% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{cart_woe}
\alias{cart_woe}
\title{CART Optimal Binning with Weight of Evidence (WoE)}
\usage{
cart_woe(
  data,
  target_col,
  feature_cols,
  min_bins = 2L,
  max_bins = 5L,
  method = "cart",
  miss_policy = "separate",
  cat_sep = "\%;\%",
  digits = 4L,
  smooth = 0,
  criterion = "gini",
  min_size = 0.05,
  use_pruning = TRUE,
  cv_folds = 5L,
  monotonic_trend = "auto",
  monotonic_mode = "pava",
  parallel = FALSE,
  weights = NULL,
  special_vals = as.numeric(c()),
  max_cat = 50L,
  rare_pct = 0.01,
  verbose = 0L
)
}
\arguments{
\item{data}{DataFrame containing features and target variable. Must contain at least the target column
and one feature column.}

\item{target_col}{String specifying the name of the binary target column. The target must be binary
(0/1, TRUE/FALSE, or two-level factor).}

\item{feature_cols}{CharacterVector of feature column names to process. These columns can be numeric,
integer, logical, character, or factor types. Date/time columns are automatically skipped.}

\item{min_bins}{Integer specifying minimum number of bins to create. Must be at least 2.
Algorithm will force creation of at least this many bins when possible.
Default: 2}

\item{max_bins}{Integer specifying maximum number of bins allowed. Must be greater than or equal to
min_bins. Controls model complexity and prevents overfitting.
Default: 6}

\item{method}{String specifying the binning algorithm to use. Options:
\itemize{
  \item \code{"cart"}: Pure CART algorithm
  \item \code{"cart+pava"}: CART with PAVA monotonicity enforcement (default)
  \item \code{"cart+merge"}: CART with heuristic merging for monotonicity
}
Default: "cart"}

\item{miss_policy}{String specifying how to handle missing values. Options:
\itemize{
  \item \code{"separate"}: Create separate bin(s) for missing values (default)
  \item \code{"remove"}: Exclude missing values from analysis
  \item \code{"impute"}: Impute with mode (categorical) or median (numeric)
  \item \code{"merge"}: Merge with nearest bin based on event rate
}
Default: "separate"}

\item{cat_sep}{String separator used when combining multiple categories into a single bin label.
Should be a pattern unlikely to appear in actual category names.
Default: "\%;\%"}

\item{digits}{Integer number of decimal places for numeric bin boundaries in labels.
Must be between 0 and 10. Affects display only, not calculation precision.
Default: 4}

\item{smooth}{Numeric Laplace smoothing factor for WoE calculation. Must be non-negative.
Prevents undefined WoE values when bins have zero counts. Higher values provide more smoothing.
Default: 0.5}

\item{criterion}{String specifying the impurity criterion for CART splits. Options:
\itemize{
  \item \code{"gini"}: Gini impurity (default)
  \item \code{"entropy"}: Information gain (entropy-based)
}
Default: "gini"}

\item{min_size}{Numeric minimum bin size specification. Can be:
\itemize{
  \item Proportion (0,1]: Minimum proportion of total observations per bin
  \item Count (>1): Minimum absolute count per bin
}
Prevents creation of very small bins that may be unstable.
Default: 0.05}

\item{use_pruning}{Logical indicating whether to apply cost-complexity pruning to the CART tree.
Helps prevent overfitting by removing branches that provide little predictive power.
Default: TRUE}

\item{cv_folds}{Integer number of cross-validation folds for pruning parameter selection.
Must be at least 2 when pruning is enabled. Higher values provide more robust pruning.
Default: 5}

\item{monotonic_trend}{String specifying the desired monotonic relationship. Options:
\itemize{
  \item \code{"auto"}: Automatically detect trend based on Spearman correlation (default)
  \item \code{"increasing"}: Enforce increasing WoE trend
  \item \code{"decreasing"}: Enforce decreasing WoE trend
  \item \code{"none"}: No monotonic constraint
}
Default: "auto"}

\item{monotonic_mode}{String specifying the method for enforcing monotonicity. Options:
\itemize{
  \item \code{"pava"}: Isotonic regression using PAVA algorithm (default)
  \item \code{"merge"}: Heuristic adjacent bin merging
}
Default: "pava"}

\item{parallel}{Logical indicating whether to use parallel processing via OpenMP for multiple features.
Requires OpenMP support in compilation. Speeds up processing of many features.
Default: FALSE}

\item{weights}{Optional NumericVector of observation weights for weighted binning. Length must equal
number of rows in data. All weights must be non-negative. NULL indicates equal weights.
Default: NULL}

\item{special_vals}{NumericVector of special values to handle separately (e.g., -999, -888 for
special codes). These values are isolated in separate bins when encountered.
Default: empty vector (no special values)}

\item{max_cat}{Integer maximum number of categories before automatic grouping for categorical variables.
Must be at least 2. Categories beyond this limit are grouped based on event rate similarity.
Default: 50}

\item{rare_pct}{Numeric threshold for rare category grouping as proportion of total.
Categories with frequency below this threshold are combined. Must be between 0 and 0.5.
Default: 0.01 (1\%)}

\item{verbose}{Integer controlling log verbosity level (0-3):
\itemize{
  \item 0: Silent mode - no logs
  \item 1: Basic logs - key steps and results  
  \item 2: Detailed logs - includes algorithm progress
  \item 3: Debug logs - full algorithm details
}
Default: 0}
}
\value{
Named list with one element per processed feature, each containing:
\itemize{
  \item \code{bins}: data.frame with columns:
    \itemize{
      \item \code{bin_id}: Sequential bin identifier (1, 2, 3, ...)
      \item \code{bin_label}: Formatted bin range/categories (e.g., "(-inf, 25.5]", "[25.5, 40.0]", "(40.0, +inf)")
      \item \code{total_count}: Total observations in bin
      \item \code{neg_count}: Count of negative class (Y=0)
      \item \code{pos_count}: Count of positive class (Y=1)
      \item \code{woe}: Weight of Evidence value for the bin
      \item \code{iv}: Information Value contribution of the bin
      \item \code{ks}: Kolmogorov-Smirnov statistic up to this bin
      \item \code{gini}: Gini coefficient for the bin
    }
  \item \code{total_iv}: Total Information Value for the feature
  \item \code{variable_type}: Detected type ("numeric_continuous", "numeric_discrete", "categorical", "boolean")
  \item \code{method_used}: Algorithm actually used for binning
  \item \code{n_bins}: Final number of bins created (including special bins if separate)
  \item \code{total_samples}: Total valid observations processed
  \item \code{event_rate}: Overall positive class rate in processed data
  \item \code{messages}: Processing messages or warnings
  \item \code{transform}: data.frame with original values and transformations:
    \itemize{
      \item \code{[feature]}: Original feature values
      \item \code{[feature]_bin}: Assigned bin labels
      \item \code{[feature]_woe}: Assigned WoE values
    }
}
}
\description{
Performs optimal binning on numeric and categorical variables using Classification and Regression Trees (CART),
computing Weight of Evidence (WoE) and Information Value (IV) for each bin. Implements comprehensive binning
strategies including:
\itemize{
  \item \strong{CART Pure}: Standard Classification and Regression Trees
  \item \strong{CART+PAVA}: CART with isotonic regression for monotonicity enforcement
  \item \strong{CART+Merge}: CART with heuristic adjacent bin merging
}
}
\details{
\strong{1) CART Algorithm for Optimal Binning}

The algorithm builds a binary tree by recursively partitioning the feature space to maximize impurity reduction:
\deqn{\Delta(t) = I(S) - \frac{W_L}{W}I(S_L) - \frac{W_R}{W}I(S_R)}
where:
\itemize{
  \item \eqn{I(S)} = impurity of parent node \eqn{S}
  \item \eqn{I(S_L), I(S_R)} = impurities of left and right child nodes
  \item \eqn{W, W_L, W_R} = total weights of parent, left child, and right child
}

\strong{Impurity Measures:}
\itemize{
  \item \strong{Gini Impurity}: \eqn{I(S) = 1 - \sum_{i=1}^{C} p_i^2}
  \item \strong{Entropy}: \eqn{I(S) = -\sum_{i=1}^{C} p_i \log(p_i)}
}
where \eqn{p_i} is the probability of class \eqn{i} in the node.

\strong{2) Weight of Evidence (WoE)}

For bin \eqn{g}, WoE measures the strength of relationship with the target:
\deqn{\text{WoE}_g = \ln\left(\frac{P(Y=1|X \in \text{bin}_g)}{P(Y=0|X \in \text{bin}_g)}\right) = \ln\left(\frac{n_{1g}/N_1}{n_{0g}/N_0}\right)}
where:
\itemize{
  \item \eqn{n_{1g}} = number of positive events in bin \eqn{g}
  \item \eqn{n_{0g}} = number of negative events in bin \eqn{g}
  \item \eqn{N_1} = total positive events
  \item \eqn{N_0} = total negative events
}

\strong{Laplace Smoothing}: To handle zero frequencies, applies smoothing factor \eqn{\lambda}:
\deqn{\text{WoE}_g = \ln\left(\frac{(n_{1g} + \lambda)/(N_1 + G\lambda)}{(n_{0g} + \lambda)/(N_0 + G\lambda)}\right)}
where \eqn{G} is the total number of bins.

\strong{3) Information Value (IV)}

Total IV quantifies the predictive power of the variable:
\deqn{\text{IV} = \sum_{g=1}^{G} \left(\frac{n_{1g}}{N_1} - \frac{n_{0g}}{N_0}\right) \times \text{WoE}_g}

Interpretation guidelines:
\itemize{
  \item IV \eqn{< 0.02}: Useless predictor
  \item \eqn{0.02 \leq} IV \eqn{< 0.1}: Weak predictor
  \item \eqn{0.1 \leq} IV \eqn{< 0.3}: Medium predictor
  \item \eqn{0.3 \leq} IV \eqn{< 0.5}: Strong predictor
  \item IV \eqn{\geq 0.5}: Suspicious (potential overfitting)
}

\strong{4) Monotonicity Enforcement}

When monotonic constraints are applied:
\itemize{
  \item \code{"cart+pava"}: Uses Pool Adjacent Violators Algorithm (PAVA) for isotonic regression
  \item \code{"cart+merge"}: Heuristic merging of adjacent bins violating monotonicity
}

\strong{PAVA Algorithm:}
Solves the isotonic regression problem:
\deqn{\min_{\theta_1 \leq \theta_2 \leq \cdots \leq \theta_G} \sum_{g=1}^{G} w_g(\theta_g - y_g)^2}
where \eqn{y_g} are target values (event rates or WoE) and \eqn{w_g} are weights.

\strong{5) Cost-Complexity Pruning}

To prevent overfitting, applies cost-complexity pruning:
\deqn{R_\alpha(T) = R(T) + \alpha|T|}
where:
\itemize{
  \item \eqn{R(T)} = total impurity of tree \eqn{T}
  \item \eqn{|T|} = number of terminal nodes (leaves)
  \item \eqn{\alpha} = complexity parameter
}

\strong{6) Special Value Handling}

Missing values and special codes are handled according to \code{miss_policy}:
\itemize{
  \item \code{"separate"}: Creates separate bin(s) for missing/special values
  \item \code{"remove"}: Excludes missing values from binning
  \item \code{"impute"}: Imputes with mode (categorical) or median (numeric)
  \item \code{"merge"}: Merges with nearest bin based on event rate
}
}
\note{
\itemize{
  \item WoE values are clipped to [-20, 20] for numerical stability
  \item Variables with 2 or fewer unique classes are processed directly without CART optimization
  \item Factor variables are processed as categorical with proper level handling
  \item Integer variables with \eqn{\leq 20} unique values are treated as discrete numeric
  \item Special values (-999, -888, etc.) can be isolated in separate bins
  \item Uses "(-inf,x)" and "[x,+inf)" notation for unbounded intervals
  \item Categorical bins combine multiple categories using \code{cat_sep} separator
}
}
\section{Algorithm Pipeline}{

The complete binning process follows these steps:
\enumerate{
  \item \strong{Data Preparation}:
    \itemize{
      \item Detect variable type (numeric, categorical, boolean, date)
      \item Handle missing values according to \code{miss_policy}
      \item Process special values into separate bins
      \item For categorical variables: order by event rate and map to ordinal scale
    }
  \item \strong{CART Tree Construction}:
    \itemize{
      \item Sort data by feature value (numeric) or ordinal mapping (categorical)
      \item Build binary tree using best-first search with impurity gain criterion
      \item Respect \code{min_size} constraints during splitting
      \item Stop when reaching \code{max_bins} leaves or no beneficial splits
    }
  \item \strong{Tree Pruning} (if enabled):
    \itemize{
      \item Generate sequence of subtrees via cost-complexity pruning
      \item Select optimal subtree using cross-validation
      \item Ensure result respects \code{min_bins} and \code{max_bins} constraints
    }
  \item \strong{Bin Adjustment}:
    \itemize{
      \item Force minimum bin count if below \code{min_bins}
      \item Merge bins if above \code{max_bins}
      \item Apply monotonicity constraints if specified
    }
  \item \strong{Metric Calculation}:
    \itemize{
      \item Compute WoE and IV with Laplace smoothing
      \item Calculate KS statistics and Gini coefficients
      \item Generate transformation mappings
    }
}
}

\section{Special Cases Handling}{

\itemize{
  \item \strong{Variables with \eqn{\le 2} unique classes}: Direct binning without optimization
  \item \strong{Date/Time variables}: Automatically skipped with warning message
  \item \strong{Unsupported types}: Skipped with appropriate message
  \item \strong{Constant variables}: Single bin created with warning
  \item \strong{Insufficient data}: Processing skipped with informative message
}
}

\examples{
\dontrun{
# Load credit data
data <- scorecard::germancredit
data$target <- ifelse(data$creditability == "good", 1, 0)

# Basic binning with all defaults
result <- cart_woe(
  data = data,
  target_col = "target",
  feature_cols = c("duration.in.month", "credit.amount", "age.in.years")
)

# Advanced binning with custom parameters
result_custom <- cart_woe(
  data = data,
  target_col = "target",
  feature_cols = names(data)[1:10],
  min_bins = 3,              # Minimum 3 bins
  max_bins = 8,              # Maximum 8 bins
  method = "cart+pava",      # CART with PAVA monotonicity
  miss_policy = "separate",  # Missing values in separate bin
  cat_sep = " | ",           # Use pipe separator for categories
  digits = 2,                # 2 decimal places in labels
  smooth = 1.0,              # Higher smoothing for WoE
  criterion = "entropy",     # Use entropy instead of Gini
  min_size = 0.03,           # Each bin at least 3\% of data
  use_pruning = TRUE,        # Enable cost-complexity pruning
  cv_folds = 10,             # 10-fold CV for pruning
  monotonic_trend = "auto",  # Auto-detect monotonic trend
  monotonic_mode = "pava",   # Use PAVA for monotonicity
  parallel = TRUE,           # Parallel processing for speed
  special_vals = c(-999, -888), # Special codes to isolate
  max_cat = 25,              # Group if >25 categories
  rare_pct = 0.02            # Group categories <2\% frequency
)

# Extract results for a specific feature
duration_bins <- result$duration.in.month$bins
duration_iv <- result$duration.in.month$total_iv
duration_transform <- result$duration.in.month$transform

# View binning results
print(duration_bins)
print(paste("Total IV:", duration_iv))

# Check transformation
head(duration_transform)
}

}
\references{
\itemize{
  \item Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). Classification and regression trees. CRC press.
  \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring and its applications. SIAM.
  \item Hand, D. J., & Henley, W. E. (1997). Statistical classification methods in consumer credit scoring: a review. Journal of the Royal Statistical Society: Series A (Statistics in Society), 160(3), 523-541.
  \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.
  \item Robertson, T., Wright, F. T., & Dykstra, R. L. (1988). Order restricted statistical inference. Wiley.
}
}
