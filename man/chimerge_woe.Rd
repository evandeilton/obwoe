% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{chimerge_woe}
\alias{chimerge_woe}
\title{ChiMerge Optimal Binning with Weight of Evidence (WoE)}
\usage{
chimerge_woe(
  data,
  target_col,
  feature_cols,
  min_bins = 2L,
  max_bins = 5L,
  sig_level = 0.05,
  min_size = 0.05,
  smooth = 0,
  monotonic = FALSE,
  min_iv = 0.01,
  digits = 4L,
  miss_policy = "separate",
  special_vals = as.numeric(c()),
  max_cat = 20L,
  rare_pct = 0.01,
  cat_sep = "\%;\%",
  method = "chimerge_mod",
  parallel = FALSE,
  cache = TRUE,
  weights = NULL,
  verbose = FALSE
)
}
\arguments{
\item{data}{DataFrame containing features and target variable. Must contain at least the target column
and one feature column.}

\item{target_col}{String specifying the name of the binary target column. The target must be binary
(0/1, TRUE/FALSE, or two-level factor).}

\item{feature_cols}{CharacterVector of feature column names to process. These columns can be numeric,
integer, logical, character, or factor types. Date/time columns are automatically skipped.}

\item{min_bins}{Integer specifying minimum number of bins to create. Must be at least 2.
Default: 2}

\item{max_bins}{Integer specifying maximum number of bins allowed. Must be greater than or equal to
min_bins. Values above 100 may lead to overfitting.
Default: 10}

\item{sig_level}{Numeric significance level for chi-square test used in ChiMerge algorithm.
Must be between 0 and 1. Common values: 0.10 (90\% confidence), 0.05 (95\% confidence),
0.01 (99\% confidence). Lower values create fewer bins.
Default: 0.05}

\item{min_size}{Numeric minimum bin size as proportion of total observations. Must be between 0 and 1.
Prevents creation of very small bins. For example, 0.05 means each bin must contain at least 5\\%
of observations.
Default: 0.05}

\item{smooth}{Numeric Laplace smoothing factor for WoE calculation. Must be non-negative.
Prevents undefined WoE values when bins have zero counts. Higher values provide more smoothing.
Default: 0.5}

\item{monotonic}{Logical indicating whether to enforce monotonic WoE values across bins using
isotonic regression. Useful for regulatory compliance and model interpretability.
Default: TRUE}

\item{min_iv}{Numeric minimum Information Value threshold for feature selection. Features with
IV below this threshold generate a warning. Must be non-negative.
Default: 0.01}

\item{digits}{Integer number of decimal places for numeric bin boundaries in labels.
Must be between 0 and 10. Affects display only, not calculation precision.
Default: 4}

\item{miss_policy}{String specifying how to handle missing values. Options:
\itemize{
  \item \code{separate}: Create separate bin(s) for missing values (default)
  \item \code{remove}: Exclude missing values from analysis
  \item \code{impute}: Impute with mode (categorical) or median (numeric)
  \item \code{merge}: Merge with nearest bin based on event rate
}
Default: separate}

\item{special_vals}{NumericVector of special values to handle separately (e.g., -999, -888 for
special codes). These values are isolated in separate bins when encountered.
Default: empty vector (no special values)}

\item{max_cat}{Integer maximum number of categories before automatic grouping for categorical variables.
Must be at least 2. Categories beyond this limit are grouped based on event rate similarity.
Default: 20}

\item{rare_pct}{Numeric threshold for rare category grouping as proportion of total.
Categories with frequency below this threshold are combined. Must be between 0 and 0.5.
Default: 0.01 (1\%)}

\item{cat_sep}{String separator used when combining multiple categories into a single bin label.
Should be a pattern unlikely to appear in actual category names.
Default: "\%;\%"}

\item{method}{String specifying the binning algorithm to use. Options:
\itemize{
  \item \code{chimerge}: Original ChiMerge algorithm
  \item \code{chimerge_mod}: Modified ChiMerge with Yates correction (default)
  \item \code{chimerge_opt}: Optimized ChiMerge with caching
  \item \code{d2}: D2 entropy-based algorithm
  \item \code{hybrid}: Hybrid approach combining multiple methods
}
Default: chimerge_mod}

\item{parallel}{Logical indicating whether to use parallel processing via OpenMP for multiple features.
Requires OpenMP support in compilation.
Default: FALSE}

\item{cache}{Logical indicating whether to enable chi-square value caching for performance optimization.
Recommended for large datasets.
Default: TRUE}

\item{weights}{Optional NumericVector of observation weights for weighted binning. Length must equal
number of rows in data. All weights must be non-negative. NULL indicates equal weights.
Default: NULL}

\item{verbose}{Logical indicating whether to print detailed processing information.
Default: FALSE}
}
\value{
Named list with one element per processed feature, each containing:
\itemize{
  \item \code{bins}: data.frame with columns:
    \itemize{
      \item \code{bin_id}: Sequential bin identifier
      \item \code{bin_label}: Formatted bin range/categories
      \item \code{total_count}: Total observations in bin
      \item \code{neg_count}: Count of negative class (Y=0)
      \item \code{pos_count}: Count of positive class (Y=1)
      \item \code{woe}: Weight of Evidence value
      \item \code{iv}: Information Value contribution
      \item \code{ks}: Kolmogorov-Smirnov statistic
      \item \code{gini}: Gini impurity coefficient
    }
  \item \code{total_iv}: Total Information Value for the feature
  \item \code{variable_type}: Detected type (numeric_continuous, numeric_discrete, categorical, boolean)
  \item \code{method_used}: Algorithm actually used for binning
  \item \code{n_bins}: Final number of bins created
  \item \code{total_samples}: Total valid observations processed
  \item \code{event_rate}: Overall positive class rate
  \item \code{messages}: Processing messages or warnings
  \item \code{transform}: data.frame with original values and transformations:
    \itemize{
      \item \code{[feature]}: Original feature values
      \item \code{[feature]_bin}: Assigned bin labels
      \item \code{[feature]_woe}: Assigned WoE values
    }
}
}
\description{
Performs optimal binning on numeric and categorical variables using multiple **ChiMerge** algorithm
variations, computing **Weight of Evidence (WoE)** and **Information Value (IV)** for each bin.
Implements comprehensive binning strategies including:
\itemize{
  \item \strong{ChiMerge Original} (Kerber, 1992): bottom-up merging based on chi-square independence test
  \item \strong{ChiMerge Modified}: handles small expected frequencies with Yates' continuity correction
  \item \strong{ChiMerge Optimized}: includes caching and performance optimizations
  \item \strong{D2 Algorithm}: entropy-based recursive splitting
  \item \strong{Hybrid WoE}: combines multiple strategies for optimal results
}
}
\details{
\strong{1) Chi-Square Test of Independence}

For adjacent intervals \eqn{i} and \eqn{j}, the chi-square statistic is:
\deqn{\chi^2 = \sum_{k=0}^{1} \sum_{l \in \\{i,j\\}} \frac{(O_{lk} - E_{lk})^2}{E_{lk}}}
where:
\itemize{
   \item \eqn{O_{lk}} = observed frequency in interval \eqn{l}, class \eqn{k}
   \item \eqn{E_{lk} = \frac{R_l \times C_k}{N}} = expected frequency under independence
   \item \eqn{R_l} = row total for interval \eqn{l}
   \item \eqn{C_k} = column total for class \eqn{k}
   \item \eqn{N} = total observations
}
With df = 1, critical values at common significance levels: \eqn{\chi^2_{0.10,1} = 2.706},
\eqn{\chi^2_{0.05,1} = 3.841}, \eqn{\chi^2_{0.01,1} = 6.635}.

\strong{2) Weight of Evidence (WoE)}

For bin \eqn{i}, WoE measures the strength of relationship with the target:
\deqn{\text{WoE}_i = \ln\left(\frac{P(X \text{ in bin}_i, Y=1)}{P(X \text{ in bin}_i, Y=0)}\right) = \ln\left(\frac{\text{Dist}_{\text{Good},i}}{\text{Dist}_{\text{Bad},i}}\right)}
where:
\deqn{\text{Dist}_{\text{Good},i} = \frac{n_{1i}}{n_1}, \quad \text{Dist}_{\text{Bad},i} = \frac{n_{0i}}{n_0}}

\strong{Laplace Smoothing}: To handle zero frequencies, applies smoothing factor \eqn{\lambda}:
\deqn{\text{WoE}_i = \ln\left(\frac{(n_{1i} + \lambda) / (n_1 + k\lambda)}{(n_{0i} + \lambda) / (n_0 + k\lambda)}\right)}
where \eqn{k} is the number of bins.

\strong{3) Information Value (IV)}

Total IV quantifies the predictive power of the variable:
\deqn{\text{IV} = \sum_{i=1}^{k} (\text{Dist}_{\text{Good},i} - \text{Dist}_{\text{Bad},i}) \times \text{WoE}_i}

Interpretation guidelines:
\itemize{
  \item IV \eqn{< 0.02}: Useless predictor
  \item \eqn{0.02 \leq} IV \eqn{< 0.1}: Weak predictor
  \item \eqn{0.1 \leq} IV \eqn{< 0.3}: Medium predictor
  \item \eqn{0.3 \leq} IV \eqn{< 0.5}: Strong predictor
  \item IV \eqn{\geq 0.5}: Suspicious (potential overfitting)
}

\strong{4) Monotonicity Enforcement}

When \code{monotonic = TRUE}, applies isotonic regression using the Pool Adjacent Violators
Algorithm (PAVA) to ensure monotonic WoE values across bins, preserving the ordinal relationship
while minimizing weighted squared deviations.

\strong{5) Special Value Handling}

Missing values and special codes are handled according to \code{miss_policy}:
\itemize{
  \item \code{separate}: Creates separate bin(s) for missing/special values
  \item \code{remove}: Excludes missing values from binning
  \item \code{impute}: Imputes with mode (categorical) or median (numeric)
  \item \code{merge}: Merges with nearest bin based on event rate
}
}
\note{
\itemize{
  \item Date/time variables are automatically excluded
  \item Factor variables are processed as categorical
  \item Integer variables with 20 or fewer unique values are treated as discrete
  \item Special values (-999, -888, etc.) can be isolated in separate bins
  \item Uses (-inf,x) and [x,+inf) notation for unbounded intervals
}
}
\section{Algorithm Details}{

The ChiMerge algorithm proceeds as follows:
\enumerate{
  \item \strong{Initialization}: Create initial bins (one per unique value for numeric, one per category for categorical)
  \item \strong{Iterative Merging}:
    \itemize{
      \item Calculate chi-square statistic for all adjacent bin pairs
      \item Find pair with minimum chi-square value
      \item If min chi-square < threshold and bins > min_bins, merge the pair
      \item Repeat until stopping criteria met
    }
  \item \strong{Post-processing}:
    \itemize{
      \item Ensure max_bins constraint
      \item Calculate WoE and IV for final bins
      \item Apply monotonicity constraint if requested
    }
}
}

\examples{
\dontrun{
# Load credit data
data <- scorecard::germancredit
data$target <- ifelse(data$creditability == "good", 1, 0)

# Basic binning with all defaults
result <- chimerge_woe(
  data = data,
  target_col = "target",
  feature_cols = c("duration.in.month", "credit.amount", "age.in.years")
)

# Custom binning with specific parameters
result_custom <- chimerge_woe(
  data = data,
  target_col = "target",
  feature_cols = names(data)[1:10],
  min_bins = 3,              # Minimum 3 bins
  max_bins = 8,               # Maximum 8 bins
  sig_level = 0.10,           # 90\% confidence level
  min_size = 0.03,            # Each bin at least 3\% of data
  smooth = 1.0,               # Higher smoothing
  monotonic = TRUE,           # Enforce monotonic WoE
  min_iv = 0.05,              # Warn if IV < 0.05
  digits = 2,                 # 2 decimal places in labels
  miss_policy = "separate",   # Missing values in separate bin
  special_vals = c(-999, -888), # Special codes to isolate
  max_cat = 15,               # Group if >15 categories
  rare_pct = 0.02,            # Group categories <2\% frequency
  cat_sep = " | ",            # Use pipe separator
  method = "chimerge_mod",    # Modified ChiMerge
  parallel = FALSE,           # No parallel processing
  cache = TRUE,               # Enable caching
  weights = NULL,             # No observation weights
  verbose = TRUE              # Show detailed logs
)

# Extract results for a specific feature
duration_bins <- result$duration.in.month$bins
duration_iv <- result$duration.in.month$total_iv
duration_transform <- result$duration.in.month$transform
}

}
\references{
\itemize{
  \item Kerber, R. (1992). ChiMerge: Discretization of numeric attributes. In Proceedings of the tenth national conference on Artificial intelligence (pp. 123-128).
  \item Liu, H., Hussain, F., Tan, C. L., & Dash, M. (2002). Discretization: An enabling technique. Data mining and knowledge discovery, 6(4), 393-423.
  \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring and its applications. SIAM.
  \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.
}
}
