% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{dmiv_woe}
\alias{dmiv_woe}
\title{Divergence Measures and Information Value (DMIV) Optimal Binning with WoE}
\usage{
dmiv_woe(
  data,
  target_col,
  feature_cols,
  min_bins = 3L,
  max_bins = 10L,
  divergence_method = "l2_norm",
  woe_method = "woe",
  smooth = 0,
  min_bin_size = 0.05,
  optimization_method = "greedy_merge",
  enforce_monotonicity = FALSE,
  monotonicity_type = "none",
  max_iterations = 1000L,
  convergence_threshold = 1e-06,
  use_cross_validation = FALSE,
  cv_folds = 5L,
  l1_regularization = 0,
  l2_regularization = 0,
  compute_confidence_intervals = FALSE,
  confidence_level = 0.95,
  parallel = FALSE,
  n_threads = -1L,
  weights = NULL,
  special_values = as.numeric(c()),
  missing_policy = "separate",
  cat_sep = "\%;\%",
  digits = 3L,
  rare_category_threshold = 0.01,
  random_seed = 42L,
  verbose = FALSE
)
}
\arguments{
\item{data}{DataFrame containing features and target variable. Must contain at least the target column
and one feature column. All data types except Date/POSIXt are supported.}

\item{target_col}{String specifying the name of the binary target column. The target must be binary
(0/1, TRUE/FALSE, or two-level factor). NA values in target result in row exclusion.}

\item{feature_cols}{CharacterVector of feature column names to process. Supports numeric, integer,
logical, character, and factor types. Date/time columns are automatically skipped with warning.}

\item{min_bins}{Integer minimum number of bins to create. Must be at least 2. Algorithm ensures
at least this many bins when data permits.
Default: 3}

\item{max_bins}{Integer maximum number of bins allowed. Must be greater than or equal to min_bins.
Controls model complexity and prevents overfitting.
Default: 10}

\item{divergence_method}{String specifying the divergence measure to optimize. Options:
\itemize{
  \item \code{"l2_norm"}: Euclidean distance (default)
  \item \code{"kullback_leibler"}: KL divergence
  \item \code{"j_divergence"}: Symmetric KL divergence
  \item \code{"hellinger"}: Hellinger distance
  \item \code{"chi_square"}: Chi-square divergence
  \item \code{"jensen_shannon"}: JS divergence
  \item \code{"total_variation"}: TV distance
  \item \code{"l1_norm"}: Manhattan distance
  \item \code{"linf_norm"}: Maximum absolute difference
}
Default: "l2_norm"}

\item{woe_method}{String specifying WoE calculation method. Options:
\itemize{
  \item \code{"woe"}: Traditional WoE with global denominators (default)
  \item \code{"woe1"}: Zeng's WOE1 with local denominators
}
Default: "woe"}

\item{smooth}{Numeric Laplace smoothing parameter for WoE/IV calculation. Must be non-negative.
Prevents undefined values when bins have zero counts. Recommended range: [0, 1].
Default: 0.0}

\item{min_bin_size}{Numeric minimum bin size as proportion of total observations. Must be in (0, 1).
Prevents creation of unstable small bins.
Default: 0.05}

\item{optimization_method}{String specifying the optimization algorithm. Options:
\itemize{
  \item \code{"greedy_merge"}: Iterative adjacent bin merging (default)
  \item \code{"dynamic_programming"}: Optimal k-bins via DP (future)
  \item \code{"branch_and_bound"}: Exhaustive search with pruning (future)
  \item \code{"simulated_annealing"}: Stochastic optimization (future)
}
Default: "greedy_merge"}

\item{enforce_monotonicity}{Logical whether to enforce monotonic WoE trend across bins.
Useful for regulatory compliance and model interpretability.
Default: FALSE}

\item{monotonicity_type}{String specifying the monotonic constraint. Options:
\itemize{
  \item \code{"none"}: No constraint (default when enforce_monotonicity=FALSE)
  \item \code{"auto"}: Detect trend via Spearman correlation
  \item \code{"increasing"}: Force non-decreasing WoE
  \item \code{"decreasing"}: Force non-increasing WoE
}
Default: "none"}

\item{max_iterations}{Integer maximum iterations for optimization algorithms. Must be positive.
Prevents infinite loops in iterative procedures.
Default: 1000}

\item{convergence_threshold}{Numeric convergence tolerance for iterative algorithms. Must be positive.
Smaller values yield more precise results but longer computation.
Default: 1e-6}

\item{use_cross_validation}{Logical whether to use cross-validation for parameter selection.
Currently reserved for future implementation.
Default: FALSE}

\item{cv_folds}{Integer number of cross-validation folds when use_cross_validation=TRUE.
Must be at least 2. Higher values provide more robust estimates.
Default: 5}

\item{l1_regularization}{Numeric L1 (Lasso) regularization strength. Must be non-negative.
Promotes sparsity in bin selection.
Default: 0.0}

\item{l2_regularization}{Numeric L2 (Ridge) regularization strength. Must be non-negative.
Prevents extreme WoE values.
Default: 0.0}

\item{compute_confidence_intervals}{Logical whether to compute Wilson score confidence intervals
for WoE values. Adds computational overhead but provides uncertainty quantification.
Default: FALSE}

\item{confidence_level}{Numeric confidence level for intervals when compute_confidence_intervals=TRUE.
Must be in (0, 1). Common values: 0.90, 0.95, 0.99.
Default: 0.95}

\item{parallel}{Logical whether to use OpenMP parallel processing for multiple features.
Requires OpenMP support in compilation. Significant speedup for many features.
Default: FALSE}

\item{n_threads}{Integer number of threads for parallel processing. Options:
\itemize{
  \item \code{-1}: Auto-detect optimal number (default)
  \item \code{1}: Sequential processing
  \item \code{2+}: Specific thread count
}
Default: -1}

\item{weights}{Optional NumericVector of observation weights for weighted binning. Length must equal
number of rows in data. All weights must be non-negative. NULL indicates unit weights.
Default: NULL}

\item{special_values}{NumericVector of special numeric codes to handle separately (e.g., -999, -888).
These values are isolated in dedicated bins regardless of their frequency.
Default: empty vector}

\item{missing_policy}{String specifying missing value handling. Options:
\itemize{
  \item \code{"separate"}: Create dedicated bin(s) for missing values (default)
  \item \code{"remove"}: Exclude missing values from analysis
  \item \code{"impute"}: Impute with median/mode (future)
  \item \code{"merge"}: Merge with nearest bin by event rate (future)
}
Default: "separate"}

\item{cat_sep}{String separator for merged categorical bin labels. Should not appear in
actual category names. Used when multiple categories are combined into one bin.
Default: "\%;\%"}

\item{digits}{Integer decimal places for numeric boundaries in bin labels. Must be in [0, 10].
Affects display precision only, not calculation accuracy.
Default: 3}

\item{rare_category_threshold}{Numeric threshold for rare category grouping as proportion.
Categories with frequency below this are combined. Must be in [0, 1].
Default: 0.01}

\item{random_seed}{Integer seed for random number generation. Ensures reproducibility
in stochastic components. Set to -1 for non-deterministic behavior.
Default: 42}

\item{verbose}{Logical whether to print detailed progress information during processing.
Useful for debugging and monitoring long-running operations.
Default: FALSE}
}
\value{
S3 object of class "dmiv_woe_result" (inheriting from "list") with:

\strong{Per-feature results} (accessed by feature name):
\itemize{
  \item \code{bins}: data.frame with columns:
    \itemize{
      \item \code{bin_id}: Sequential bin identifier (1, 2, 3, ...)
      \item \code{bin_label}: Formatted bin description (e.g., "(-inf, 25.5]", "cat1\%;\%cat2")
      \item \code{total_count}: Total observations in bin
      \item \code{neg_count}: Count of negative class (Y=0)
      \item \code{pos_count}: Count of positive class (Y=1)
      \item \code{woe}: Weight of Evidence value (clipped to [-20, 20])
      \item \code{divergence}: Divergence measure contribution
      \item \code{iv}: Information Value contribution
      \item \code{ks}: Kolmogorov-Smirnov statistic
      \item \code{gini}: Gini coefficient for the bin
    }
  \item \code{total_divergence}: Total divergence across all bins
  \item \code{total_iv}: Total Information Value for the feature
  \item \code{variable_type}: Detected type ("numeric_continuous", "numeric_discrete", "categorical", "boolean")
  \item \code{n_bins}: Final number of bins created
  \item \code{messages}: Processing status or error messages
  \item \code{divergence_method}: Divergence measure used
  \item \code{transform}: data.frame with transformations:
    \itemize{
      \item \code{[feature]}: Original feature values
      \item \code{[feature]_bin}: Assigned bin labels
      \item \code{[feature]_woe}: Assigned WoE values
    }
}

\strong{Attributes}:
\itemize{
  \item \code{config}: List of all configuration parameters used
  \item \code{summary}: Processing summary with:
    \itemize{
      \item \code{n_features}: Total features processed
      \item \code{n_success}: Successfully processed features
      \item \code{n_errors}: Features with errors
      \item \code{processing_time_seconds}: Total computation time
      \item \code{timestamp}: Processing timestamp (format: "YYYY-MM-DD HH:MM:SS")
    }
}
}
\description{
Performs state-of-the-art optimal binning on numeric and categorical variables using advanced
divergence measures combined with Information Value optimization. Implements comprehensive binning
strategies that extend traditional approaches through:
\itemize{
  \item \strong{Multi-Divergence Optimization}: Simultaneous optimization of multiple divergence measures
  \item \strong{Regularized Information Value}: L1/L2 regularization for robust binning
  \item \strong{Adaptive Monotonicity}: Isotonic regression with automatic trend detection
  \item \strong{Fast Transform Mapping}: O(log n) bin assignment using optimized data structures
}
}
\details{
\strong{1) Divergence Measures Framework}

The algorithm optimizes bin boundaries by minimizing divergence between positive and negative class distributions:
\deqn{D(P||Q) = \sum_{i=1}^{n} d(p_i, q_i)}
where \eqn{P} and \eqn{Q} are the probability distributions for positive and negative classes.

\strong{Supported Divergence Measures:}
\itemize{
  \item \strong{L2 Norm (Euclidean)}: \eqn{D_{L2} = \sqrt{\sum_{i=1}^{n}(p_i - q_i)^2}}
  \item \strong{Kullback-Leibler}: \eqn{D_{KL} = \sum_{i=1}^{n} p_i \log\left(\frac{p_i}{q_i}\right)}
  \item \strong{J-Divergence}: \eqn{D_J = \sum_{i=1}^{n} (p_i - q_i) \log\left(\frac{p_i}{q_i}\right)}
  \item \strong{Hellinger Distance}: \eqn{D_H = \frac{1}{2}\sum_{i=1}^{n}(\sqrt{p_i} - \sqrt{q_i})^2}
  \item \strong{Chi-Square}: \eqn{D_{\chi^2} = \sum_{i=1}^{n} \frac{(p_i - q_i)^2(p_i + q_i)}{p_i \cdot q_i}}
  \item \strong{Jensen-Shannon}: \eqn{D_{JS} = \frac{1}{2}D_{KL}(P||M) + \frac{1}{2}D_{KL}(Q||M)}, where \eqn{M = \frac{P+Q}{2}}
  \item \strong{Total Variation}: \eqn{D_{TV} = \frac{1}{2}\sum_{i=1}^{n}|p_i - q_i|}
  \item \strong{L1 Norm}: \eqn{D_{L1} = \sum_{i=1}^{n}|p_i - q_i|}
  \item \strong{Linf Norm}: \eqn{D_{L\infty} = \max_{i}|p_i - q_i|}
}

\strong{2) Weight of Evidence with Advanced Smoothing}

For bin \eqn{g}, WoE with configurable smoothing:
\deqn{\text{WoE}_g = \begin{cases}
\ln\left(\frac{(n_{1g} + \lambda)/(N_1 + G\lambda)}{(n_{0g} + \lambda)/(N_0 + G\lambda)}\right) & \text{if method = "woe"} \\
\ln\left(\frac{(n_{1g} + \lambda)/(n_g + 2\lambda)}{(n_{0g} + \lambda)/(n_g + 2\lambda)}\right) & \text{if method = "woe1"}
\end{cases}}
where:
\itemize{
  \item \eqn{n_{1g}, n_{0g}} = positive/negative counts in bin \eqn{g}
  \item \eqn{N_1, N_0} = total positive/negative counts
  \item \eqn{n_g} = total count in bin \eqn{g}
  \item \eqn{G} = number of bins
  \item \eqn{\lambda} = Laplace smoothing parameter
}

WoE values are clipped to \eqn{[-20, 20]} for numerical stability.

\strong{3) Regularized Information Value}

Total IV with regularization:
\deqn{\text{IV}_{\text{reg}} = \sum_{g=1}^{G} \left(\frac{n_{1g}}{N_1} - \frac{n_{0g}}{N_0}\right) \times \text{WoE}_g + \lambda_1 ||w||_1 + \lambda_2 ||w||_2^2}
where \eqn{\lambda_1} and \eqn{\lambda_2} are L1 and L2 regularization parameters.

\strong{4) Optimization Algorithms}

\strong{Greedy Merge Algorithm:}
\enumerate{
  \item Initialize with maximum granular bins (3× max_bins)
  \item Iteratively merge adjacent bins with minimum IV loss
  \item Continue until max_bins constraint is satisfied
  \item Convergence criterion: \eqn{|D_t - D_{t-1}| < \epsilon}
}

\strong{Dynamic Programming (Future):}
Solves optimal k-bins problem:
\deqn{OPT[i,k] = \min_{j<i} \{OPT[j,k-1] + \text{Cost}(j+1,i)\}}

\strong{5) Isotonic Regression for Monotonicity}

Pool Adjacent Violators Algorithm (PAVA):
\deqn{\min_{\theta} \sum_{i=1}^{n} w_i(\theta_i - y_i)^2 \quad \text{s.t.} \quad \theta_1 \leq \theta_2 \leq \cdots \leq \theta_n}

Algorithm complexity: \eqn{O(n)} using efficient pooling strategy.

\strong{6) Wilson Score Confidence Intervals}

For WoE confidence intervals at level \eqn{1-\alpha}:
\deqn{\text{CI} = \frac{\hat{p} + \frac{z^2}{2n} \pm z\sqrt{\frac{\hat{p}(1-\hat{p})}{n} + \frac{z^2}{4n^2}}}{1 + \frac{z^2}{n}}}
where \eqn{z = \Phi^{-1}(1-\alpha/2)} is the standard normal quantile.

\strong{7) Fast Bin Mapping}

Utilizes optimized data structures for O(log n) bin assignment:
\itemize{
  \item \strong{Numeric}: Binary search on sorted thresholds
  \item \strong{Categorical}: Hash map lookup with O(1) average complexity
}
}
\section{Algorithm Pipeline}{

The complete DMIV optimization process:
\enumerate{
  \item \strong{Type Detection and Validation}:
    \itemize{
      \item Automatic variable type inference (numeric/categorical/boolean)
      \item Discrete numeric detection (<= 20 unique values)
      \item Date/time variable filtering with warnings
    }
  \item \strong{Data Preprocessing}:
    \itemize{
      \item Missing value handling per \code{missing_policy}
      \item Special value isolation into dedicated bins
      \item Rare category grouping below \code{rare_category_threshold}
      \item Weight normalization if provided
    }
  \item \strong{Initial Binning}:
    \itemize{
      \item Numeric: Quantile-based splitting (3× max_bins granularity)
      \item Categorical: Event rate ordering with rare grouping
      \item Boolean: Direct two-bin assignment
    }
  \item \strong{Optimization Phase}:
    \itemize{
      \item Iterative bin merging to minimize divergence
      \item Regularization penalty application
      \item Convergence monitoring with early stopping
      \item Constraint satisfaction (min/max bins, min size)
    }
  \item \strong{Monotonicity Enforcement} (if enabled):
    \itemize{
      \item Trend detection via Spearman correlation
      \item PAVA isotonic regression application
      \item WoE recalculation post-adjustment
    }
  \item \strong{Metric Computation}:
    \itemize{
      \item WoE calculation with chosen method and smoothing
      \item IV computation with regularization
      \item KS and Gini statistics
      \item Confidence intervals (if requested)
    }
  \item \strong{Fast Transform Generation}:
    \itemize{
      \item Build optimized mapping structures
      \item Apply transformations to original data
      \item Generate output DataFrames
    }
}
}

\section{Performance Characteristics}{

\itemize{
  \item \strong{Time Complexity}: \eqn{O(n \log n + kb^2)} where n=observations, k=iterations, b=bins
  \item \strong{Space Complexity}: \eqn{O(n + b^2)} for data storage and optimization matrices
  \item \strong{Parallelization}: Linear speedup with thread count for multiple features
  \item \strong{Numerical Stability}: All logarithms use safe_log with \eqn{\epsilon = 10^{-12}}
}
}

\section{Implementation Notes}{

\itemize{
  \item Uses C++17 features including structured bindings and std::atomic
  \item OpenMP parallelization with dynamic scheduling for load balancing
  \item Thread-safe bin statistics using atomic operations
  \item Memory-efficient Eigen library for matrix operations
  \item Fast bin mapping via binary search (numeric) and hash maps (categorical)
  \item Extensive input validation with informative error messages
}
}

\examples{
\dontrun{
# Load sample credit data
library(scorecard)
data <- germancredit
data$target <- ifelse(data$creditability == "good", 1, 0)

# Basic usage with defaults
result <- dmiv_woe(
  data = data,
  target_col = "target",
  feature_cols = c("duration.in.month", "credit.amount", "age.in.years")
)

# Advanced configuration with multiple divergence measures
result_advanced <- dmiv_woe(
  data = data,
  target_col = "target",
  feature_cols = names(data)[1:20],
  min_bins = 2,                    # Minimum 2 bins
  max_bins = 8,                     # Maximum 8 bins
  divergence_method = "hellinger",  # Hellinger distance
  woe_method = "woe1",              # Zeng's WOE1
  smooth = 0.5,                     # Moderate smoothing
  min_bin_size = 0.03,              # 3\% minimum size
  optimization_method = "greedy_merge",
  enforce_monotonicity = TRUE,      # Enforce monotonic WoE
  monotonicity_type = "auto",       # Auto-detect trend
  max_iterations = 2000,            # More iterations
  convergence_threshold = 1e-8,     # Tighter convergence
  l1_regularization = 0.01,         # L1 penalty
  l2_regularization = 0.001,        # L2 penalty
  compute_confidence_intervals = TRUE,  # Calculate CIs
  confidence_level = 0.99,          # 99\% confidence
  parallel = TRUE,                  # Use parallelization
  n_threads = 4,                    # 4 threads
  special_values = c(-999, -888),   # Special codes
  missing_policy = "separate",      # Separate missing bin
  cat_sep = " | ",                  # Pipe separator
  digits = 2,                       # 2 decimal places
  rare_category_threshold = 0.005,  # 0.5\% rare threshold
  random_seed = 123,                # For reproducibility
  verbose = TRUE                    # Show progress
)

# Extract and analyze results
duration_result <- result_advanced$duration.in.month
print(duration_result$bins)
print(paste("Total IV:", round(duration_result$total_iv, 4)))
print(paste("Total Divergence:", round(duration_result$total_divergence, 4)))

# Apply transformation to new data
transform_df <- duration_result$transform
head(transform_df)

# Compare divergence methods
methods <- c("l2_norm", "hellinger", "kullback_leibler", "jensen_shannon")
comparison <- lapply(methods, function(m) {
  res <- dmiv_woe(data, "target", "credit.amount", divergence_method = m)
  list(method = m,
       n_bins = res$credit.amount$n_bins,
       total_iv = res$credit.amount$total_iv,
       total_div = res$credit.amount$total_divergence)
})
do.call(rbind, lapply(comparison, as.data.frame))

# Process categorical variables
cat_result <- dmiv_woe(
  data = data,
  target_col = "target",
  feature_cols = c("purpose", "personal.status.sex"),
  max_bins = 5,
  rare_category_threshold = 0.02,
  cat_sep = " + "
)

# Examine categorical binning
print(cat_result$purpose$bins)
}

}
\references{
\itemize{
  \item Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulation. arXiv preprint arXiv:2001.08025.
  \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.
  \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. Working Paper.
  \item Cover, T. M., & Thomas, J. A. (2006). Elements of information theory. John Wiley & Sons.
  \item Robertson, T., Wright, F. T., & Dykstra, R. L. (1988). Order restricted statistical inference. Wiley.
  \item Wilson, E. B. (1927). Probable inference, the law of succession, and statistical inference. JASA, 22(158), 209-212.
}
}
\seealso{
\code{\link{cart_woe}} for CART-based binning,
\code{\link{optbin}} for alternative optimization approaches
}
