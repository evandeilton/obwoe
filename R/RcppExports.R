# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' ChiMerge Optimal Binning with Weight of Evidence (WoE)
#'
#' @description
#' Performs optimal binning on numeric and categorical variables using multiple **ChiMerge** algorithm
#' variations, computing **Weight of Evidence (WoE)** and **Information Value (IV)** for each bin.
#' Implements comprehensive binning strategies including:
#' \itemize{
#'   \item \strong{ChiMerge Original} (Kerber, 1992): bottom-up merging based on chi-square independence test
#'   \item \strong{ChiMerge Modified}: handles small expected frequencies with Yates' continuity correction
#'   \item \strong{ChiMerge Optimized}: includes caching and performance optimizations
#'   \item \strong{D2 Algorithm}: entropy-based recursive splitting
#'   \item \strong{Hybrid WoE}: combines multiple strategies for optimal results
#' }
#'
#' @details
#' \strong{1) Chi-Square Test of Independence}
#'
#' For adjacent intervals \eqn{i} and \eqn{j}, the chi-square statistic is:
#' \deqn{\chi^2 = \sum_{k=0}^{1} \sum_{l \in \\{i,j\\}} \frac{(O_{lk} - E_{lk})^2}{E_{lk}}}
#' where:
#' \itemize{
#'    \item \eqn{O_{lk}} = observed frequency in interval \eqn{l}, class \eqn{k}
#'    \item \eqn{E_{lk} = \frac{R_l \times C_k}{N}} = expected frequency under independence
#'    \item \eqn{R_l} = row total for interval \eqn{l}
#'    \item \eqn{C_k} = column total for class \eqn{k}
#'    \item \eqn{N} = total observations
#' }
#' With df = 1, critical values at common significance levels: \eqn{\chi^2_{0.10,1} = 2.706},
#' \eqn{\chi^2_{0.05,1} = 3.841}, \eqn{\chi^2_{0.01,1} = 6.635}.
#'
#' \strong{2) Weight of Evidence (WoE)}
#'
#' For bin \eqn{i}, WoE measures the strength of relationship with the target:
#' \deqn{\text{WoE}_i = \ln\left(\frac{P(X \text{ in bin}_i, Y=1)}{P(X \text{ in bin}_i, Y=0)}\right) = \ln\left(\frac{\text{Dist}_{\text{Good},i}}{\text{Dist}_{\text{Bad},i}}\right)}
#' where:
#' \deqn{\text{Dist}_{\text{Good},i} = \frac{n_{1i}}{n_1}, \quad \text{Dist}_{\text{Bad},i} = \frac{n_{0i}}{n_0}}
#'
#' \strong{Laplace Smoothing}: To handle zero frequencies, applies smoothing factor \eqn{\lambda}:
#' \deqn{\text{WoE}_i = \ln\left(\frac{(n_{1i} + \lambda) / (n_1 + k\lambda)}{(n_{0i} + \lambda) / (n_0 + k\lambda)}\right)}
#' where \eqn{k} is the number of bins.
#'
#' \strong{3) Information Value (IV)}
#'
#' Total IV quantifies the predictive power of the variable:
#' \deqn{\text{IV} = \sum_{i=1}^{k} (\text{Dist}_{\text{Good},i} - \text{Dist}_{\text{Bad},i}) \times \text{WoE}_i}
#'
#' Interpretation guidelines:
#' \itemize{
#'   \item IV \eqn{< 0.02}: Useless predictor
#'   \item \eqn{0.02 \leq} IV \eqn{< 0.1}: Weak predictor
#'   \item \eqn{0.1 \leq} IV \eqn{< 0.3}: Medium predictor
#'   \item \eqn{0.3 \leq} IV \eqn{< 0.5}: Strong predictor
#'   \item IV \eqn{\geq 0.5}: Suspicious (potential overfitting)
#' }
#'
#' \strong{4) Monotonicity Enforcement}
#'
#' When \code{monotonic = TRUE}, applies isotonic regression using the Pool Adjacent Violators
#' Algorithm (PAVA) to ensure monotonic WoE values across bins, preserving the ordinal relationship
#' while minimizing weighted squared deviations.
#'
#' \strong{5) Special Value Handling}
#'
#' Missing values and special codes are handled according to \code{miss_policy}:
#' \itemize{
#'   \item \code{separate}: Creates separate bin(s) for missing/special values
#'   \item \code{remove}: Excludes missing values from binning
#'   \item \code{impute}: Imputes with mode (categorical) or median (numeric)
#'   \item \code{merge}: Merges with nearest bin based on event rate
#' }
#'
#' @param data DataFrame containing features and target variable. Must contain at least the target column
#'   and one feature column.
#'
#' @param target_col String specifying the name of the binary target column. The target must be binary
#'   (0/1, TRUE/FALSE, or two-level factor).
#'
#' @param feature_cols CharacterVector of feature column names to process. These columns can be numeric,
#'   integer, logical, character, or factor types. Date/time columns are automatically skipped.
#'
#' @param min_bins Integer specifying minimum number of bins to create. Must be at least 2.
#'   Default: 2
#'
#' @param max_bins Integer specifying maximum number of bins allowed. Must be greater than or equal to
#'   min_bins. Values above 100 may lead to overfitting.
#'   Default: 10
#'
#' @param sig_level Numeric significance level for chi-square test used in ChiMerge algorithm.
#'   Must be between 0 and 1. Common values: 0.10 (90\% confidence), 0.05 (95\% confidence),
#'   0.01 (99\% confidence). Lower values create fewer bins.
#'   Default: 0.05
#'
#' @param min_size Numeric minimum bin size as proportion of total observations. Must be between 0 and 1.
#'   Prevents creation of very small bins. For example, 0.05 means each bin must contain at least 5\\%
#'   of observations.
#'   Default: 0.05
#'
#' @param smooth Numeric Laplace smoothing factor for WoE calculation. Must be non-negative.
#'   Prevents undefined WoE values when bins have zero counts. Higher values provide more smoothing.
#'   Default: 0.5
#'
#' @param monotonic Logical indicating whether to enforce monotonic WoE values across bins using
#'   isotonic regression. Useful for regulatory compliance and model interpretability.
#'   Default: TRUE
#'
#' @param min_iv Numeric minimum Information Value threshold for feature selection. Features with
#'   IV below this threshold generate a warning. Must be non-negative.
#'   Default: 0.01
#'
#' @param digits Integer number of decimal places for numeric bin boundaries in labels.
#'   Must be between 0 and 10. Affects display only, not calculation precision.
#'   Default: 4
#'
#' @param miss_policy String specifying how to handle missing values. Options:
#'   \itemize{
#'     \item \code{separate}: Create separate bin(s) for missing values (default)
#'     \item \code{remove}: Exclude missing values from analysis
#'     \item \code{impute}: Impute with mode (categorical) or median (numeric)
#'     \item \code{merge}: Merge with nearest bin based on event rate
#'   }
#'   Default: separate
#'
#' @param special_vals NumericVector of special values to handle separately (e.g., -999, -888 for
#'   special codes). These values are isolated in separate bins when encountered.
#'   Default: empty vector (no special values)
#'
#' @param max_cat Integer maximum number of categories before automatic grouping for categorical variables.
#'   Must be at least 2. Categories beyond this limit are grouped based on event rate similarity.
#'   Default: 20
#'
#' @param rare_pct Numeric threshold for rare category grouping as proportion of total.
#'   Categories with frequency below this threshold are combined. Must be between 0 and 0.5.
#'   Default: 0.01 (1\%)
#'
#' @param cat_sep String separator used when combining multiple categories into a single bin label.
#'   Should be a pattern unlikely to appear in actual category names.
#'   Default: "\%;\%"
#'
#' @param method String specifying the binning algorithm to use. Options:
#'   \itemize{
#'     \item \code{chimerge}: Original ChiMerge algorithm
#'     \item \code{chimerge_mod}: Modified ChiMerge with Yates correction (default)
#'     \item \code{chimerge_opt}: Optimized ChiMerge with caching
#'     \item \code{d2}: D2 entropy-based algorithm
#'     \item \code{hybrid}: Hybrid approach combining multiple methods
#'   }
#'   Default: chimerge_mod
#'
#' @param parallel Logical indicating whether to use parallel processing via OpenMP for multiple features.
#'   Requires OpenMP support in compilation.
#'   Default: FALSE
#'
#' @param cache Logical indicating whether to enable chi-square value caching for performance optimization.
#'   Recommended for large datasets.
#'   Default: TRUE
#'
#' @param weights Optional NumericVector of observation weights for weighted binning. Length must equal
#'   number of rows in data. All weights must be non-negative. NULL indicates equal weights.
#'   Default: NULL
#'
#' @param verbose Logical indicating whether to print detailed processing information.
#'   Default: FALSE
#'
#' @return
#' Named list with one element per processed feature, each containing:
#' \itemize{
#'   \item \code{bins}: data.frame with columns:
#'     \itemize{
#'       \item \code{bin_id}: Sequential bin identifier
#'       \item \code{bin_label}: Formatted bin range/categories
#'       \item \code{total_count}: Total observations in bin
#'       \item \code{neg_count}: Count of negative class (Y=0)
#'       \item \code{pos_count}: Count of positive class (Y=1)
#'       \item \code{woe}: Weight of Evidence value
#'       \item \code{iv}: Information Value contribution
#'       \item \code{ks}: Kolmogorov-Smirnov statistic
#'       \item \code{gini}: Gini impurity coefficient
#'     }
#'   \item \code{total_iv}: Total Information Value for the feature
#'   \item \code{variable_type}: Detected type (numeric_continuous, numeric_discrete, categorical, boolean)
#'   \item \code{method_used}: Algorithm actually used for binning
#'   \item \code{n_bins}: Final number of bins created
#'   \item \code{total_samples}: Total valid observations processed
#'   \item \code{event_rate}: Overall positive class rate
#'   \item \code{messages}: Processing messages or warnings
#'   \item \code{transform}: data.frame with original values and transformations:
#'     \itemize{
#'       \item \code{[feature]}: Original feature values
#'       \item \code{[feature]_bin}: Assigned bin labels
#'       \item \code{[feature]_woe}: Assigned WoE values
#'     }
#' }
#'
#' @section Algorithm Details:
#' The ChiMerge algorithm proceeds as follows:
#' \enumerate{
#'   \item \strong{Initialization}: Create initial bins (one per unique value for numeric, one per category for categorical)
#'   \item \strong{Iterative Merging}:
#'     \itemize{
#'       \item Calculate chi-square statistic for all adjacent bin pairs
#'       \item Find pair with minimum chi-square value
#'       \item If min chi-square < threshold and bins > min_bins, merge the pair
#'       \item Repeat until stopping criteria met
#'     }
#'   \item \strong{Post-processing}:
#'     \itemize{
#'       \item Ensure max_bins constraint
#'       \item Calculate WoE and IV for final bins
#'       \item Apply monotonicity constraint if requested
#'     }
#' }
#'
#' @note
#' \itemize{
#'   \item Date/time variables are automatically excluded
#'   \item Factor variables are processed as categorical
#'   \item Integer variables with 20 or fewer unique values are treated as discrete
#'   \item Special values (-999, -888, etc.) can be isolated in separate bins
#'   \item Uses (-inf,x) and [x,+inf) notation for unbounded intervals
#' }
#'
#' @references
#' \itemize{
#'   \item Kerber, R. (1992). ChiMerge: Discretization of numeric attributes. In Proceedings of the tenth national conference on Artificial intelligence (pp. 123-128).
#'   \item Liu, H., Hussain, F., Tan, C. L., & Dash, M. (2002). Discretization: An enabling technique. Data mining and knowledge discovery, 6(4), 393-423.
#'   \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring and its applications. SIAM.
#'   \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.
#' }
#'
#' @examples
#' \dontrun{
#' # Load credit data
#' data <- scorecard::germancredit
#' data$target <- ifelse(data$creditability == "good", 1, 0)
#'
#' # Basic binning with all defaults
#' result <- chimerge_woe(
#'   data = data,
#'   target_col = "target",
#'   feature_cols = c("duration.in.month", "credit.amount", "age.in.years")
#' )
#'
#' # Custom binning with specific parameters
#' result_custom <- chimerge_woe(
#'   data = data,
#'   target_col = "target",
#'   feature_cols = names(data)[1:10],
#'   min_bins = 3,              # Minimum 3 bins
#'   max_bins = 8,               # Maximum 8 bins
#'   sig_level = 0.10,           # 90% confidence level
#'   min_size = 0.03,            # Each bin at least 3% of data
#'   smooth = 1.0,               # Higher smoothing
#'   monotonic = TRUE,           # Enforce monotonic WoE
#'   min_iv = 0.05,              # Warn if IV < 0.05
#'   digits = 2,                 # 2 decimal places in labels
#'   miss_policy = "separate",   # Missing values in separate bin
#'   special_vals = c(-999, -888), # Special codes to isolate
#'   max_cat = 15,               # Group if >15 categories
#'   rare_pct = 0.02,            # Group categories <2% frequency
#'   cat_sep = " | ",            # Use pipe separator
#'   method = "chimerge_mod",    # Modified ChiMerge
#'   parallel = FALSE,           # No parallel processing
#'   cache = TRUE,               # Enable caching
#'   weights = NULL,             # No observation weights
#'   verbose = TRUE              # Show detailed logs
#' )
#'
#' # Extract results for a specific feature
#' duration_bins <- result$duration.in.month$bins
#' duration_iv <- result$duration.in.month$total_iv
#' duration_transform <- result$duration.in.month$transform
#' }
#'
#' @export
chimerge_woe <- function(data, target_col, feature_cols, min_bins = 2L, max_bins = 5L, sig_level = 0.05, min_size = 0.05, smooth = 0, monotonic = FALSE, min_iv = 0.01, digits = 4L, miss_policy = "separate", special_vals = as.numeric( c()), max_cat = 20L, rare_pct = 0.01, cat_sep = "%;%", method = "chimerge_mod", parallel = FALSE, cache = TRUE, weights = NULL, verbose = FALSE) {
    .Call(`_obwoe_chimerge_woe`, data, target_col, feature_cols, min_bins, max_bins, sig_level, min_size, smooth, monotonic, min_iv, digits, miss_policy, special_vals, max_cat, rare_pct, cat_sep, method, parallel, cache, weights, verbose)
}

#' Gains Table Analysis for WoE Binning Results
#'
#' @description
#' Function to compute comprehensive gains table metrics from WoE binning results. Calculates over 20 statistical
#' measures for evaluating binning quality, predictive power, and model performance.
#'
#' The gains table provides detailed insights into how well each bin separates positive and negative cases,
#' enabling assessment of binning effectiveness and feature predictive power.
#'
#' @details
#' \strong{1) Core Metrics}
#'
#' \emph{Basic Counts and Rates}:
#' \itemize{
#'   \item \code{total_count}, \code{pos_count}, \code{neg_count}: Raw counts per bin
#'   \item \code{event_rate}: Proportion of positive cases in each bin
#'   \item \code{pct_total}: Percentage of total population in each bin
#' }
#'
#' \emph{Cumulative Statistics}:
#' \itemize{
#'   \item \code{cum_pos}: Cumulative percentage of positive cases (True Positive Rate/Sensitivity)
#'   \item \code{cum_neg}: Cumulative percentage of negative cases (False Positive Rate)
#'   \item \code{cum_event_rate}: Cumulative event rate
#' }
#'
#' \strong{2) Performance Metrics}
#'
#' \emph{Lift and Capture}:
#' \deqn{\text{Lift} = \frac{\text{Event Rate}}{\text{Base Rate}}}
#' \deqn{\text{Capture Rate} = \frac{\text{Cumulative Positives}}{\text{Total Positives}}}
#' where Base Rate is the overall proportion of positive cases.
#'
#' \emph{KS Statistic}:
#' \deqn{\text{KS} = \max|\text{Cumulative Positives} - \text{Cumulative Negatives}|}
#' measuring the maximum separation between positive and negative distributions.
#'
#' \emph{ROC Analysis}:
#' Area Under Curve (AUC) is calculated using the trapezoidal rule on the ROC curve:
#' \deqn{\text{AUC} = \sum_{i=1}^{n} \frac{\text{TPR}_i + \text{TPR}_{i-1}}{2} \times (\text{FPR}_{i-1} - \text{FPR}_i)}
#'
#' \emph{Classification Metrics}:
#' \itemize{
#'   \item \code{precision} (Positive Predictive Value): \eqn{\frac{TP}{TP + FP}}
#'   \item \code{sensitivity} (True Positive Rate): \eqn{\frac{TP}{TP + FN}}
#'   \item \code{specificity} (True Negative Rate): \eqn{\frac{TN}{TN + FP}}
#'   \item \code{F1 Score}: \eqn{2 \times \frac{\text{precision} \times \text{sensitivity}}{\text{precision} + \text{sensitivity}}}
#'   \item \code{Matthews Correlation Coefficient}: \eqn{\frac{TP \times TN - FP \times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}}
#' }
#'
#' \strong{3) Information Theory Metrics}
#'
#' \emph{Entropy}:
#' \deqn{H(p) = -p \log_2(p) - (1-p) \log_2(1-p)}
#' where \eqn{p} is the event rate in the bin.
#'
#' \emph{Kullback-Leibler Divergence}:
#' \deqn{D_{KL}(p||q) = p \log\left(\frac{p}{q}\right) + (1-p) \log\left(\frac{1-p}{1-q}\right)}
#' measuring divergence from the base rate \eqn{q}.
#'
#' \emph{Information Value Component}:
#' \deqn{\text{IV}_i = \left(\frac{\text{Count}_{1i}}{\text{Total}_1} - \frac{\text{Count}_{0i}}{\text{Total}_0}\right) \times \text{WoE}_i}
#'
#' \strong{4) Statistical Tests}
#'
#' \emph{Chi-Square Test} for each bin against the null hypothesis of independence.
#'
#' \emph{Z-Score} for testing if bin event rate differs significantly from base rate:
#' \deqn{Z = \frac{p_1 - p_2}{\sqrt{p_{\text{pooled}}(1-p_{\text{pooled}})(\frac{1}{n_1} + \frac{1}{n_2})}}}
#' where \eqn{p_{\text{pooled}} = \frac{p_1 n_1 + p_2 n_2}{n_1 + n_2}}.
#'
#' @param binning_result list output from \code{chimerge_woe}
#' @param sort_by_woe Logical, whether to sort bins by WoE (default: FALSE)
#' @param include_all_metrics Logical, whether to include all metrics or just essential ones (default: TRUE)
#' @param selected_features Optional character vector of features to process (default: NULL)
#' @param verbose Logical indicating whether to print detailed processing information (default: FALSE)
#'
#' @return
#' list with one element per feature, each containing:
#' \itemize{
#'   \item \code{gains_table}: data.frame with detailed metrics for each bin
#'   \item \code{summary_stats}: list with overall statistics
#'   \item \code{variable_type}: String, variable type
#'   \item \code{sorted_by}: String, sorting method used
#' }
#'
#' The \code{gains_table} includes the following metrics:
#' \itemize{
#'   \item \strong{Basic Information}: bin_id, bin_label, counts, woe, iv
#'   \item \strong{Percentages}: pct_total, pct_pos, pct_neg, event_rate
#'   \item \strong{Cumulative Statistics}: cum_total, cum_pos, cum_neg, cum_event_rate
#'   \item \strong{Lift Metrics}: lift, cum_lift, capture_rate
#'   \item \strong{KS and Gini}: ks, gini
#'   \item \strong{Entropy and Information}: entropy, divergence, info_gain
#'   \item \strong{Risk Metrics}: odds, log_odds, relative_risk
#'   \item \strong{Performance Metrics}: precision, npv, f1_score, accuracy, balanced_acc, specificity
#'   \item \strong{Statistical Tests}: chi_square, z_score, p_value
#'   \item \strong{Stability and Concentration}: psi_component, concentration_ratio
#'   \item \strong{Additional Metrics}: miss_rate, fall_out, discovery_rate, mcc
#' }
#'
#' The \code{summary_stats} includes:
#' \itemize{
#'   \item \code{base_rate}: Overall event rate
#'   \item \code{total_observations}: Total number of observations
#'   \item \code{max_ks}: Maximum KS statistic
#'   \item \code{auc}: Area Under ROC Curve
#'   \item \code{gini_coefficient}: Gini coefficient (2 * AUC - 1)
#'   \item \code{total_psi}: Total Population Stability Index
#'   \item \code{herfindahl_index}: Herfindahl-Hirschman Index
#'   \item \code{normalized_herfindahl}: Normalized HHI
#'   \item \code{total_iv}: Total Information Value
#'   \item \code{n_bins}: Number of bins
#' }
#'
#' @section Best practices for interpretation:
#' \itemize{
#'   \item \strong{KS Statistic}: Values > 0.4 indicate good separation power
#'   \item \strong{AUC}: Values > 0.7 indicate acceptable discrimination
#'   \item \strong{Lift}: Values > 2 in top deciles indicate good targeting
#'   \item \strong{Capture Rate}: Should increase monotonically with cumulative percentage
#'   \item \strong{IV Components}: Large positive/negative values indicate strong predictive bins
#' }
#'
#' @seealso
#' \itemize{
#'   \item \code{chimerge_woe} for binning
#'   \item \code{woe_gains_compare} for feature comparison
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. Wiley.
#'   \item Anderson, R. (2007). The Credit Scoring Toolkit: Theory and Practice for Retail Credit Risk Management and Decision Automation. Oxford University Press.
#'   \item Hand, D. J., & Henley, W. E. (1997). Statistical Classification Methods in Consumer Credit. Journal of the Royal Statistical Society, Series A, 160(3), 523-541.
#'   \item Kolmogorov, A. N. (1933). Sulla determinazione empirica di una legge di distribuzione. Giornale dell'Istituto Italiano degli Attuari, 4, 83-91.
#'   \item Smirnov, N. V. (1948). Table for Estimating the Goodness of Fit of Empirical Distributions. Annals of Mathematical Statistics, 19(2), 279-281.
#' }
#'
#' @examples
#' \dontrun{
#' # Assuming binning_result from chimerge_woe
#' gains_result <- woe_gains(binning_result, sort_by_woe = FALSE)
#'
#' # View gains table for first feature
#' head(gains_result[[1]]$gains_table)
#'
#' # View summary statistics
#' gains_result[[1]]$summary_stats
#'
#' # Compare features using essential metrics only
#' simple_gains <- woe_gains(binning_result, include_all_metrics = FALSE)
#' }
#'
#' @keywords gains-table woe iv ks-statistic auc roc
#' @export
woe_gains <- function(binning_result, sort_by_woe = FALSE, include_all_metrics = TRUE, selected_features = NULL, verbose = FALSE) {
    .Call(`_obwoe_woe_gains`, binning_result, sort_by_woe, include_all_metrics, selected_features, verbose)
}

#' Compare Features by Gains Table Metrics
#'
#' @description
#' Function to rank and compare features based on various gains table metrics. Provides a consolidated
#' view of feature performance for model selection and variable screening.
#'
#' @details
#' This function aggregates summary statistics from gains tables and ranks features according to
#' a selected performance metric. It's particularly useful for:
#' \itemize{
#'   \item Feature selection based on predictive power
#'   \item Model validation and comparison
#'   \item Monitoring feature stability over time
#'   \item Business reporting and dashboard creation
#' }
#'
#' The comparison includes several key metrics:
#' \itemize{
#'   \item \code{max_ks}: Maximum Kolmogorov-Smirnov statistic
#'   \item \code{auc}: Area Under ROC Curve
#'   \item \code{gini}: Gini coefficient (\eqn{2 \times} AUC \eqn{- 1})
#'   \item \code{total_iv}: Total Information Value
#'   \item \code{psi}: Population Stability Index
#' }
#'
#' When the selected metric is "iv", the column containing the metric values is named "selected_metric"
#' to avoid naming conflicts with the "total_iv" column.
#'
#' @param gains_results \code{list} output from \code{\link{woe_gains}}
#' @param metric String, metric to use for ranking (default: "max_ks")
#' @param descending Logical, whether to sort in descending order (default: TRUE)
#'
#' @return
#' \code{data.frame} with features ranked by the selected metric:
#' \itemize{
#'   \item \code{rank}: Integer ranking position
#'   \item \code{feature}: String feature name
#'   \item \code{[metric]}: Double value of selected metric (named according to metric parameter)
#'   \item \code{total_iv}: Double total Information Value
#'   \item \code{max_ks}: Double maximum KS statistic
#'   \item \code{auc}: Double Area Under Curve
#'   \item \code{gini}: Double Gini coefficient
#'   \item \code{psi}: Double Population Stability Index
#'   \item \code{n_bins}: Integer number of bins
#' }
#'
#' @section Metric Interpretation:
#' \itemize{
#'   \item \strong{max_ks}: Higher values indicate better separation (range: 0-1)
#'   \item \strong{auc}: Higher values indicate better discrimination (range: 0.5-1)
#'   \item \strong{gini}: Higher values indicate better ranking power (range: 0-1)
#'   \item \strong{total_iv}: Higher values indicate stronger predictive power
#'   \item \strong{psi}: Lower values indicate better stability (rule of thumb: \eqn{< 0.1} stable, 0.1-0.2 moderate, \eqn{> 0.2} unstable)
#' }
#'
#' @seealso
#' \itemize{
#'   \item \code{\link{chimerge_woe}} for binning
#'   \item \code{\link{woe_gains}} for detailed gains analysis
#' }
#'
#' @references
#' \itemize{
#'   \item Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. Wiley.
#'   \item Powers, D. M. (2011). Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness and Correlation. Journal of Machine Learning Technologies, 2(1), 37-63.
#' }
#'
#' @examples
#' \dontrun{
#' # Assuming gains_result from woe_gains
#' 
#' # Rank by KS statistic (default)
#' ks_ranking <- woe_gains_compare(gains_result)
#' 
#' # Rank by Information Value
#' iv_ranking <- woe_gains_compare(gains_result, metric = "iv")
#' 
#' # Rank by AUC in ascending order
#' auc_ranking <- woe_gains_compare(gains_result, metric = "auc", descending = FALSE)
#' 
#' # View top 10 features by IV
#' head(iv_ranking, 10)
#' }
#'
#' @keywords feature-selection model-validation ranking comparison
#' @export
woe_gains_compare <- function(gains_results, metric = "max_ks", descending = TRUE) {
    .Call(`_obwoe_woe_gains_compare`, gains_results, metric, descending)
}

#' Divergence Measures and Information Value (DMIV) Optimal Binning with WoE
#'
#' @description
#' Performs state-of-the-art optimal binning on numeric and categorical variables using advanced
#' divergence measures combined with Information Value optimization. Implements comprehensive binning
#' strategies that extend traditional approaches through:
#' \itemize{
#'   \item \strong{Multi-Divergence Optimization}: Simultaneous optimization of multiple divergence measures
#'   \item \strong{Regularized Information Value}: L1/L2 regularization for robust binning
#'   \item \strong{Adaptive Monotonicity}: Isotonic regression with automatic trend detection
#'   \item \strong{Fast Transform Mapping}: O(log n) bin assignment using optimized data structures
#' }
#'
#' @details
#' \strong{1) Divergence Measures Framework}
#'
#' The algorithm optimizes bin boundaries by minimizing divergence between positive and negative class distributions:
#' \deqn{D(P||Q) = \sum_{i=1}^{n} d(p_i, q_i)}
#' where \eqn{P} and \eqn{Q} are the probability distributions for positive and negative classes.
#'
#' \strong{Supported Divergence Measures:}
#' \itemize{
#'   \item \strong{L2 Norm (Euclidean)}: \eqn{D_{L2} = \sqrt{\sum_{i=1}^{n}(p_i - q_i)^2}}
#'   \item \strong{Kullback-Leibler}: \eqn{D_{KL} = \sum_{i=1}^{n} p_i \log\left(\frac{p_i}{q_i}\right)}
#'   \item \strong{J-Divergence}: \eqn{D_J = \sum_{i=1}^{n} (p_i - q_i) \log\left(\frac{p_i}{q_i}\right)}
#'   \item \strong{Hellinger Distance}: \eqn{D_H = \frac{1}{2}\sum_{i=1}^{n}(\sqrt{p_i} - \sqrt{q_i})^2}
#'   \item \strong{Chi-Square}: \eqn{D_{\chi^2} = \sum_{i=1}^{n} \frac{(p_i - q_i)^2(p_i + q_i)}{p_i \cdot q_i}}
#'   \item \strong{Jensen-Shannon}: \eqn{D_{JS} = \frac{1}{2}D_{KL}(P||M) + \frac{1}{2}D_{KL}(Q||M)}, where \eqn{M = \frac{P+Q}{2}}
#'   \item \strong{Total Variation}: \eqn{D_{TV} = \frac{1}{2}\sum_{i=1}^{n}|p_i - q_i|}
#'   \item \strong{L1 Norm}: \eqn{D_{L1} = \sum_{i=1}^{n}|p_i - q_i|}
#'   \item \strong{Linf Norm}: \eqn{D_{L\infty} = \max_{i}|p_i - q_i|}
#' }
#'
#' \strong{2) Weight of Evidence with Advanced Smoothing}
#'
#' For bin \eqn{g}, WoE with configurable smoothing:
#' \deqn{\text{WoE}_g = \begin{cases}
#' \ln\left(\frac{(n_{1g} + \lambda)/(N_1 + G\lambda)}{(n_{0g} + \lambda)/(N_0 + G\lambda)}\right) & \text{if method = "woe"} \\
#' \ln\left(\frac{(n_{1g} + \lambda)/(n_g + 2\lambda)}{(n_{0g} + \lambda)/(n_g + 2\lambda)}\right) & \text{if method = "woe1"}
#' \end{cases}}
#' where:
#' \itemize{
#'   \item \eqn{n_{1g}, n_{0g}} = positive/negative counts in bin \eqn{g}
#'   \item \eqn{N_1, N_0} = total positive/negative counts
#'   \item \eqn{n_g} = total count in bin \eqn{g}
#'   \item \eqn{G} = number of bins
#'   \item \eqn{\lambda} = Laplace smoothing parameter
#' }
#'
#' WoE values are clipped to \eqn{[-20, 20]} for numerical stability.
#'
#' \strong{3) Regularized Information Value}
#'
#' Total IV with regularization:
#' \deqn{\text{IV}_{\text{reg}} = \sum_{g=1}^{G} \left(\frac{n_{1g}}{N_1} - \frac{n_{0g}}{N_0}\right) \times \text{WoE}_g + \lambda_1 ||w||_1 + \lambda_2 ||w||_2^2}
#' where \eqn{\lambda_1} and \eqn{\lambda_2} are L1 and L2 regularization parameters.
#'
#' \strong{4) Optimization Algorithms}
#'
#' \strong{Greedy Merge Algorithm:}
#' \enumerate{
#'   \item Initialize with maximum granular bins (3× max_bins)
#'   \item Iteratively merge adjacent bins with minimum IV loss
#'   \item Continue until max_bins constraint is satisfied
#'   \item Convergence criterion: \eqn{|D_t - D_{t-1}| < \epsilon}
#' }
#'
#' \strong{Dynamic Programming (Future):}
#' Solves optimal k-bins problem:
#' \deqn{OPT[i,k] = \min_{j<i} \{OPT[j,k-1] + \text{Cost}(j+1,i)\}}
#'
#' \strong{5) Isotonic Regression for Monotonicity}
#'
#' Pool Adjacent Violators Algorithm (PAVA):
#' \deqn{\min_{\theta} \sum_{i=1}^{n} w_i(\theta_i - y_i)^2 \quad \text{s.t.} \quad \theta_1 \leq \theta_2 \leq \cdots \leq \theta_n}
#'
#' Algorithm complexity: \eqn{O(n)} using efficient pooling strategy.
#'
#' \strong{6) Wilson Score Confidence Intervals}
#'
#' For WoE confidence intervals at level \eqn{1-\alpha}:
#' \deqn{\text{CI} = \frac{\hat{p} + \frac{z^2}{2n} \pm z\sqrt{\frac{\hat{p}(1-\hat{p})}{n} + \frac{z^2}{4n^2}}}{1 + \frac{z^2}{n}}}
#' where \eqn{z = \Phi^{-1}(1-\alpha/2)} is the standard normal quantile.
#'
#' \strong{7) Fast Bin Mapping}
#'
#' Utilizes optimized data structures for O(log n) bin assignment:
#' \itemize{
#'   \item \strong{Numeric}: Binary search on sorted thresholds
#'   \item \strong{Categorical}: Hash map lookup with O(1) average complexity
#' }
#'
#' @param data DataFrame containing features and target variable. Must contain at least the target column
#'   and one feature column. All data types except Date/POSIXt are supported.
#'
#' @param target_col String specifying the name of the binary target column. The target must be binary
#'   (0/1, TRUE/FALSE, or two-level factor). NA values in target result in row exclusion.
#'
#' @param feature_cols CharacterVector of feature column names to process. Supports numeric, integer,
#'   logical, character, and factor types. Date/time columns are automatically skipped with warning.
#'
#' @param min_bins Integer minimum number of bins to create. Must be at least 2. Algorithm ensures
#'   at least this many bins when data permits.
#'   Default: 3
#'
#' @param max_bins Integer maximum number of bins allowed. Must be greater than or equal to min_bins.
#'   Controls model complexity and prevents overfitting.
#'   Default: 10
#'
#' @param divergence_method String specifying the divergence measure to optimize. Options:
#'   \itemize{
#'     \item \code{"l2_norm"}: Euclidean distance (default)
#'     \item \code{"kullback_leibler"}: KL divergence
#'     \item \code{"j_divergence"}: Symmetric KL divergence
#'     \item \code{"hellinger"}: Hellinger distance
#'     \item \code{"chi_square"}: Chi-square divergence
#'     \item \code{"jensen_shannon"}: JS divergence
#'     \item \code{"total_variation"}: TV distance
#'     \item \code{"l1_norm"}: Manhattan distance
#'     \item \code{"linf_norm"}: Maximum absolute difference
#'   }
#'   Default: "l2_norm"
#'
#' @param woe_method String specifying WoE calculation method. Options:
#'   \itemize{
#'     \item \code{"woe"}: Traditional WoE with global denominators (default)
#'     \item \code{"woe1"}: Zeng's WOE1 with local denominators
#'   }
#'   Default: "woe"
#'
#' @param smooth Numeric Laplace smoothing parameter for WoE/IV calculation. Must be non-negative.
#'   Prevents undefined values when bins have zero counts. Recommended range: [0, 1].
#'   Default: 0.0
#'
#' @param min_bin_size Numeric minimum bin size as proportion of total observations. Must be in (0, 1).
#'   Prevents creation of unstable small bins.
#'   Default: 0.05
#'
#' @param optimization_method String specifying the optimization algorithm. Options:
#'   \itemize{
#'     \item \code{"greedy_merge"}: Iterative adjacent bin merging (default)
#'     \item \code{"dynamic_programming"}: Optimal k-bins via DP (future)
#'     \item \code{"branch_and_bound"}: Exhaustive search with pruning (future)
#'     \item \code{"simulated_annealing"}: Stochastic optimization (future)
#'   }
#'   Default: "greedy_merge"
#'
#' @param enforce_monotonicity Logical whether to enforce monotonic WoE trend across bins.
#'   Useful for regulatory compliance and model interpretability.
#'   Default: FALSE
#'
#' @param monotonicity_type String specifying the monotonic constraint. Options:
#'   \itemize{
#'     \item \code{"none"}: No constraint (default when enforce_monotonicity=FALSE)
#'     \item \code{"auto"}: Detect trend via Spearman correlation
#'     \item \code{"increasing"}: Force non-decreasing WoE
#'     \item \code{"decreasing"}: Force non-increasing WoE
#'   }
#'   Default: "none"
#'
#' @param max_iterations Integer maximum iterations for optimization algorithms. Must be positive.
#'   Prevents infinite loops in iterative procedures.
#'   Default: 1000
#'
#' @param convergence_threshold Numeric convergence tolerance for iterative algorithms. Must be positive.
#'   Smaller values yield more precise results but longer computation.
#'   Default: 1e-6
#'
#' @param use_cross_validation Logical whether to use cross-validation for parameter selection.
#'   Currently reserved for future implementation.
#'   Default: FALSE
#'
#' @param cv_folds Integer number of cross-validation folds when use_cross_validation=TRUE.
#'   Must be at least 2. Higher values provide more robust estimates.
#'   Default: 5
#'
#' @param l1_regularization Numeric L1 (Lasso) regularization strength. Must be non-negative.
#'   Promotes sparsity in bin selection.
#'   Default: 0.0
#'
#' @param l2_regularization Numeric L2 (Ridge) regularization strength. Must be non-negative.
#'   Prevents extreme WoE values.
#'   Default: 0.0
#'
#' @param compute_confidence_intervals Logical whether to compute Wilson score confidence intervals
#'   for WoE values. Adds computational overhead but provides uncertainty quantification.
#'   Default: FALSE
#'
#' @param confidence_level Numeric confidence level for intervals when compute_confidence_intervals=TRUE.
#'   Must be in (0, 1). Common values: 0.90, 0.95, 0.99.
#'   Default: 0.95
#'
#' @param parallel Logical whether to use OpenMP parallel processing for multiple features.
#'   Requires OpenMP support in compilation. Significant speedup for many features.
#'   Default: FALSE
#'
#' @param n_threads Integer number of threads for parallel processing. Options:
#'   \itemize{
#'     \item \code{-1}: Auto-detect optimal number (default)
#'     \item \code{1}: Sequential processing
#'     \item \code{2+}: Specific thread count
#'   }
#'   Default: -1
#'
#' @param weights Optional NumericVector of observation weights for weighted binning. Length must equal
#'   number of rows in data. All weights must be non-negative. NULL indicates unit weights.
#'   Default: NULL
#'
#' @param special_values NumericVector of special numeric codes to handle separately (e.g., -999, -888).
#'   These values are isolated in dedicated bins regardless of their frequency.
#'   Default: empty vector
#'
#' @param missing_policy String specifying missing value handling. Options:
#'   \itemize{
#'     \item \code{"separate"}: Create dedicated bin(s) for missing values (default)
#'     \item \code{"remove"}: Exclude missing values from analysis
#'     \item \code{"impute"}: Impute with median/mode (future)
#'     \item \code{"merge"}: Merge with nearest bin by event rate (future)
#'   }
#'   Default: "separate"
#'
#' @param cat_sep String separator for merged categorical bin labels. Should not appear in
#'   actual category names. Used when multiple categories are combined into one bin.
#'   Default: "\%;\%"
#'
#' @param digits Integer decimal places for numeric boundaries in bin labels. Must be in [0, 10].
#'   Affects display precision only, not calculation accuracy.
#'   Default: 3
#'
#' @param rare_category_threshold Numeric threshold for rare category grouping as proportion.
#'   Categories with frequency below this are combined. Must be in [0, 1].
#'   Default: 0.01
#'
#' @param random_seed Integer seed for random number generation. Ensures reproducibility
#'   in stochastic components. Set to -1 for non-deterministic behavior.
#'   Default: 42
#'
#' @param verbose Logical whether to print detailed progress information during processing.
#'   Useful for debugging and monitoring long-running operations.
#'   Default: FALSE
#'
#' @return
#' S3 object of class "dmiv_woe_result" (inheriting from "list") with:
#'
#' \strong{Per-feature results} (accessed by feature name):
#' \itemize{
#'   \item \code{bins}: data.frame with columns:
#'     \itemize{
#'       \item \code{bin_id}: Sequential bin identifier (1, 2, 3, ...)
#'       \item \code{bin_label}: Formatted bin description (e.g., "(-inf, 25.5]", "cat1\%;\%cat2")
#'       \item \code{total_count}: Total observations in bin
#'       \item \code{neg_count}: Count of negative class (Y=0)
#'       \item \code{pos_count}: Count of positive class (Y=1)
#'       \item \code{woe}: Weight of Evidence value (clipped to [-20, 20])
#'       \item \code{divergence}: Divergence measure contribution
#'       \item \code{iv}: Information Value contribution
#'       \item \code{ks}: Kolmogorov-Smirnov statistic
#'       \item \code{gini}: Gini coefficient for the bin
#'     }
#'   \item \code{total_divergence}: Total divergence across all bins
#'   \item \code{total_iv}: Total Information Value for the feature
#'   \item \code{variable_type}: Detected type ("numeric_continuous", "numeric_discrete", "categorical", "boolean")
#'   \item \code{n_bins}: Final number of bins created
#'   \item \code{messages}: Processing status or error messages
#'   \item \code{divergence_method}: Divergence measure used
#'   \item \code{transform}: data.frame with transformations:
#'     \itemize{
#'       \item \code{[feature]}: Original feature values
#'       \item \code{[feature]_bin}: Assigned bin labels
#'       \item \code{[feature]_woe}: Assigned WoE values
#'     }
#' }
#'
#' \strong{Attributes}:
#' \itemize{
#'   \item \code{config}: List of all configuration parameters used
#'   \item \code{summary}: Processing summary with:
#'     \itemize{
#'       \item \code{n_features}: Total features processed
#'       \item \code{n_success}: Successfully processed features
#'       \item \code{n_errors}: Features with errors
#'       \item \code{processing_time_seconds}: Total computation time
#'       \item \code{timestamp}: Processing timestamp (format: "YYYY-MM-DD HH:MM:SS")
#'     }
#' }
#'
#' @section Algorithm Pipeline:
#' The complete DMIV optimization process:
#' \enumerate{
#'   \item \strong{Type Detection and Validation}:
#'     \itemize{
#'       \item Automatic variable type inference (numeric/categorical/boolean)
#'       \item Discrete numeric detection (<= 20 unique values)
#'       \item Date/time variable filtering with warnings
#'     }
#'   \item \strong{Data Preprocessing}:
#'     \itemize{
#'       \item Missing value handling per \code{missing_policy}
#'       \item Special value isolation into dedicated bins
#'       \item Rare category grouping below \code{rare_category_threshold}
#'       \item Weight normalization if provided
#'     }
#'   \item \strong{Initial Binning}:
#'     \itemize{
#'       \item Numeric: Quantile-based splitting (3× max_bins granularity)
#'       \item Categorical: Event rate ordering with rare grouping
#'       \item Boolean: Direct two-bin assignment
#'     }
#'   \item \strong{Optimization Phase}:
#'     \itemize{
#'       \item Iterative bin merging to minimize divergence
#'       \item Regularization penalty application
#'       \item Convergence monitoring with early stopping
#'       \item Constraint satisfaction (min/max bins, min size)
#'     }
#'   \item \strong{Monotonicity Enforcement} (if enabled):
#'     \itemize{
#'       \item Trend detection via Spearman correlation
#'       \item PAVA isotonic regression application
#'       \item WoE recalculation post-adjustment
#'     }
#'   \item \strong{Metric Computation}:
#'     \itemize{
#'       \item WoE calculation with chosen method and smoothing
#'       \item IV computation with regularization
#'       \item KS and Gini statistics
#'       \item Confidence intervals (if requested)
#'     }
#'   \item \strong{Fast Transform Generation}:
#'     \itemize{
#'       \item Build optimized mapping structures
#'       \item Apply transformations to original data
#'       \item Generate output DataFrames
#'     }
#' }
#'
#' @section Performance Characteristics:
#' \itemize{
#'   \item \strong{Time Complexity}: \eqn{O(n \log n + kb^2)} where n=observations, k=iterations, b=bins
#'   \item \strong{Space Complexity}: \eqn{O(n + b^2)} for data storage and optimization matrices
#'   \item \strong{Parallelization}: Linear speedup with thread count for multiple features
#'   \item \strong{Numerical Stability}: All logarithms use safe_log with \eqn{\epsilon = 10^{-12}}
#' }
#'
#' @section Implementation Notes:
#' \itemize{
#'   \item Uses C++17 features including structured bindings and std::atomic
#'   \item OpenMP parallelization with dynamic scheduling for load balancing
#'   \item Thread-safe bin statistics using atomic operations
#'   \item Memory-efficient Eigen library for matrix operations
#'   \item Fast bin mapping via binary search (numeric) and hash maps (categorical)
#'   \item Extensive input validation with informative error messages
#' }
#'
#' @references
#' \itemize{
#'   \item Navas-Palencia, G. (2020). Optimal binning: mathematical programming formulation. arXiv preprint arXiv:2001.08025.
#'   \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.
#'   \item Mironchyk, P., & Tchistiakov, V. (2017). Monotone optimal binning algorithm for credit risk modeling. Working Paper.
#'   \item Cover, T. M., & Thomas, J. A. (2006). Elements of information theory. John Wiley & Sons.
#'   \item Robertson, T., Wright, F. T., & Dykstra, R. L. (1988). Order restricted statistical inference. Wiley.
#'   \item Wilson, E. B. (1927). Probable inference, the law of succession, and statistical inference. JASA, 22(158), 209-212.
#' }
#'
#' @examples
#' \dontrun{
#' # Load sample credit data
#' library(scorecard)
#' data <- germancredit
#' data$target <- ifelse(data$creditability == "good", 1, 0)
#'
#' # Basic usage with defaults
#' result <- dmiv_woe(
#'   data = data,
#'   target_col = "target",
#'   feature_cols = c("duration.in.month", "credit.amount", "age.in.years")
#' )
#'
#' # Advanced configuration with multiple divergence measures
#' result_advanced <- dmiv_woe(
#'   data = data,
#'   target_col = "target",
#'   feature_cols = names(data)[1:20],
#'   min_bins = 2,                    # Minimum 2 bins
#'   max_bins = 8,                     # Maximum 8 bins
#'   divergence_method = "hellinger",  # Hellinger distance
#'   woe_method = "woe1",              # Zeng's WOE1
#'   smooth = 0.5,                     # Moderate smoothing
#'   min_bin_size = 0.03,              # 3% minimum size
#'   optimization_method = "greedy_merge",
#'   enforce_monotonicity = TRUE,      # Enforce monotonic WoE
#'   monotonicity_type = "auto",       # Auto-detect trend
#'   max_iterations = 2000,            # More iterations
#'   convergence_threshold = 1e-8,     # Tighter convergence
#'   l1_regularization = 0.01,         # L1 penalty
#'   l2_regularization = 0.001,        # L2 penalty
#'   compute_confidence_intervals = TRUE,  # Calculate CIs
#'   confidence_level = 0.99,          # 99% confidence
#'   parallel = TRUE,                  # Use parallelization
#'   n_threads = 4,                    # 4 threads
#'   special_values = c(-999, -888),   # Special codes
#'   missing_policy = "separate",      # Separate missing bin
#'   cat_sep = " | ",                  # Pipe separator
#'   digits = 2,                       # 2 decimal places
#'   rare_category_threshold = 0.005,  # 0.5% rare threshold
#'   random_seed = 123,                # For reproducibility
#'   verbose = TRUE                    # Show progress
#' )
#'
#' # Extract and analyze results
#' duration_result <- result_advanced$duration.in.month
#' print(duration_result$bins)
#' print(paste("Total IV:", round(duration_result$total_iv, 4)))
#' print(paste("Total Divergence:", round(duration_result$total_divergence, 4)))
#'
#' # Apply transformation to new data
#' transform_df <- duration_result$transform
#' head(transform_df)
#'
#' # Compare divergence methods
#' methods <- c("l2_norm", "hellinger", "kullback_leibler", "jensen_shannon")
#' comparison <- lapply(methods, function(m) {
#'   res <- dmiv_woe(data, "target", "credit.amount", divergence_method = m)
#'   list(method = m,
#'        n_bins = res$credit.amount$n_bins,
#'        total_iv = res$credit.amount$total_iv,
#'        total_div = res$credit.amount$total_divergence)
#' })
#' do.call(rbind, lapply(comparison, as.data.frame))
#'
#' # Process categorical variables
#' cat_result <- dmiv_woe(
#'   data = data,
#'   target_col = "target",
#'   feature_cols = c("purpose", "personal.status.sex"),
#'   max_bins = 5,
#'   rare_category_threshold = 0.02,
#'   cat_sep = " + "
#' )
#'
#' # Examine categorical binning
#' print(cat_result$purpose$bins)
#' }
#'
#' @seealso
#' \code{\link{cart_woe}} for CART-based binning,
#' \code{\link{optbin}} for alternative optimization approaches
#'
#' @export
dmiv_woe <- function(data, target_col, feature_cols, min_bins = 3L, max_bins = 10L, divergence_method = "l2_norm", woe_method = "woe", smooth = 0.0, min_bin_size = 0.05, optimization_method = "greedy_merge", enforce_monotonicity = FALSE, monotonicity_type = "none", max_iterations = 1000L, convergence_threshold = 1e-6, use_cross_validation = FALSE, cv_folds = 5L, l1_regularization = 0.0, l2_regularization = 0.0, compute_confidence_intervals = FALSE, confidence_level = 0.95, parallel = FALSE, n_threads = -1L, weights = NULL, special_values = as.numeric( c()), missing_policy = "separate", cat_sep = "%;%", digits = 3L, rare_category_threshold = 0.01, random_seed = 42L, verbose = FALSE) {
    .Call(`_obwoe_dmiv_woe`, data, target_col, feature_cols, min_bins, max_bins, divergence_method, woe_method, smooth, min_bin_size, optimization_method, enforce_monotonicity, monotonicity_type, max_iterations, convergence_threshold, use_cross_validation, cv_folds, l1_regularization, l2_regularization, compute_confidence_intervals, confidence_level, parallel, n_threads, weights, special_values, missing_policy, cat_sep, digits, rare_category_threshold, random_seed, verbose)
}

#' CART Optimal Binning with Weight of Evidence (WoE)
#'
#' @description
#' Performs optimal binning on numeric and categorical variables using Classification and Regression Trees (CART),
#' computing Weight of Evidence (WoE) and Information Value (IV) for each bin. Implements comprehensive binning
#' strategies including:
#' \itemize{
#'   \item \strong{CART Pure}: Standard Classification and Regression Trees
#'   \item \strong{CART+PAVA}: CART with isotonic regression for monotonicity enforcement
#'   \item \strong{CART+Merge}: CART with heuristic adjacent bin merging
#' }
#'
#' @details
#' \strong{1) CART Algorithm for Optimal Binning}
#'
#' The algorithm builds a binary tree by recursively partitioning the feature space to maximize impurity reduction:
#' \deqn{\Delta(t) = I(S) - \frac{W_L}{W}I(S_L) - \frac{W_R}{W}I(S_R)}
#' where:
#' \itemize{
#'   \item \eqn{I(S)} = impurity of parent node \eqn{S}
#'   \item \eqn{I(S_L), I(S_R)} = impurities of left and right child nodes
#'   \item \eqn{W, W_L, W_R} = total weights of parent, left child, and right child
#' }
#'
#' \strong{Impurity Measures:}
#' \itemize{
#'   \item \strong{Gini Impurity}: \eqn{I(S) = 1 - \sum_{i=1}^{C} p_i^2}
#'   \item \strong{Entropy}: \eqn{I(S) = -\sum_{i=1}^{C} p_i \log(p_i)}
#' }
#' where \eqn{p_i} is the probability of class \eqn{i} in the node.
#'
#' \strong{2) Weight of Evidence (WoE)}
#'
#' For bin \eqn{g}, WoE measures the strength of relationship with the target:
#' \deqn{\text{WoE}_g = \ln\left(\frac{P(Y=1|X \in \text{bin}_g)}{P(Y=0|X \in \text{bin}_g)}\right) = \ln\left(\frac{n_{1g}/N_1}{n_{0g}/N_0}\right)}
#' where:
#' \itemize{
#'   \item \eqn{n_{1g}} = number of positive events in bin \eqn{g}
#'   \item \eqn{n_{0g}} = number of negative events in bin \eqn{g}
#'   \item \eqn{N_1} = total positive events
#'   \item \eqn{N_0} = total negative events
#' }
#'
#' \strong{Laplace Smoothing}: To handle zero frequencies, applies smoothing factor \eqn{\lambda}:
#' \deqn{\text{WoE}_g = \ln\left(\frac{(n_{1g} + \lambda)/(N_1 + G\lambda)}{(n_{0g} + \lambda)/(N_0 + G\lambda)}\right)}
#' where \eqn{G} is the total number of bins.
#'
#' \strong{3) Information Value (IV)}
#'
#' Total IV quantifies the predictive power of the variable:
#' \deqn{\text{IV} = \sum_{g=1}^{G} \left(\frac{n_{1g}}{N_1} - \frac{n_{0g}}{N_0}\right) \times \text{WoE}_g}
#'
#' Interpretation guidelines:
#' \itemize{
#'   \item IV \eqn{< 0.02}: Useless predictor
#'   \item \eqn{0.02 \leq} IV \eqn{< 0.1}: Weak predictor
#'   \item \eqn{0.1 \leq} IV \eqn{< 0.3}: Medium predictor
#'   \item \eqn{0.3 \leq} IV \eqn{< 0.5}: Strong predictor
#'   \item IV \eqn{\geq 0.5}: Suspicious (potential overfitting)
#' }
#'
#' \strong{4) Monotonicity Enforcement}
#'
#' When monotonic constraints are applied:
#' \itemize{
#'   \item \code{"cart+pava"}: Uses Pool Adjacent Violators Algorithm (PAVA) for isotonic regression
#'   \item \code{"cart+merge"}: Heuristic merging of adjacent bins violating monotonicity
#' }
#'
#' \strong{PAVA Algorithm:}
#' Solves the isotonic regression problem:
#' \deqn{\min_{\theta_1 \leq \theta_2 \leq \cdots \leq \theta_G} \sum_{g=1}^{G} w_g(\theta_g - y_g)^2}
#' where \eqn{y_g} are target values (event rates or WoE) and \eqn{w_g} are weights.
#'
#' \strong{5) Cost-Complexity Pruning}
#'
#' To prevent overfitting, applies cost-complexity pruning:
#' \deqn{R_\alpha(T) = R(T) + \alpha|T|}
#' where:
#' \itemize{
#'   \item \eqn{R(T)} = total impurity of tree \eqn{T}
#'   \item \eqn{|T|} = number of terminal nodes (leaves)
#'   \item \eqn{\alpha} = complexity parameter
#' }
#'
#' \strong{6) Special Value Handling}
#'
#' Missing values and special codes are handled according to \code{miss_policy}:
#' \itemize{
#'   \item \code{"separate"}: Creates separate bin(s) for missing/special values
#'   \item \code{"remove"}: Excludes missing values from binning
#'   \item \code{"impute"}: Imputes with mode (categorical) or median (numeric)
#'   \item \code{"merge"}: Merges with nearest bin based on event rate
#' }
#'
#' @param data DataFrame containing features and target variable. Must contain at least the target column
#'   and one feature column.
#'
#' @param target_col String specifying the name of the binary target column. The target must be binary
#'   (0/1, TRUE/FALSE, or two-level factor).
#'
#' @param feature_cols CharacterVector of feature column names to process. These columns can be numeric,
#'   integer, logical, character, or factor types. Date/time columns are automatically skipped.
#'
#' @param min_bins Integer specifying minimum number of bins to create. Must be at least 2.
#'   Algorithm will force creation of at least this many bins when possible.
#'   Default: 2
#'
#' @param max_bins Integer specifying maximum number of bins allowed. Must be greater than or equal to
#'   min_bins. Controls model complexity and prevents overfitting.
#'   Default: 6
#'
#' @param method String specifying the binning algorithm to use. Options:
#'   \itemize{
#'     \item \code{"cart"}: Pure CART algorithm
#'     \item \code{"cart+pava"}: CART with PAVA monotonicity enforcement (default)
#'     \item \code{"cart+merge"}: CART with heuristic merging for monotonicity
#'   }
#'   Default: "cart"
#'
#' @param miss_policy String specifying how to handle missing values. Options:
#'   \itemize{
#'     \item \code{"separate"}: Create separate bin(s) for missing values (default)
#'     \item \code{"remove"}: Exclude missing values from analysis
#'     \item \code{"impute"}: Impute with mode (categorical) or median (numeric)
#'     \item \code{"merge"}: Merge with nearest bin based on event rate
#'   }
#'   Default: "separate"
#'
#' @param cat_sep String separator used when combining multiple categories into a single bin label.
#'   Should be a pattern unlikely to appear in actual category names.
#'   Default: "\%;\%"
#'
#' @param digits Integer number of decimal places for numeric bin boundaries in labels.
#'   Must be between 0 and 10. Affects display only, not calculation precision.
#'   Default: 4
#'
#' @param smooth Numeric Laplace smoothing factor for WoE calculation. Must be non-negative.
#'   Prevents undefined WoE values when bins have zero counts. Higher values provide more smoothing.
#'   Default: 0.5
#'
#' @param criterion String specifying the impurity criterion for CART splits. Options:
#'   \itemize{
#'     \item \code{"gini"}: Gini impurity (default)
#'     \item \code{"entropy"}: Information gain (entropy-based)
#'   }
#'   Default: "gini"
#'
#' @param min_size Numeric minimum bin size specification. Can be:
#'   \itemize{
#'     \item Proportion (0,1]: Minimum proportion of total observations per bin
#'     \item Count (>1): Minimum absolute count per bin
#'   }
#'   Prevents creation of very small bins that may be unstable.
#'   Default: 0.05
#'
#' @param use_pruning Logical indicating whether to apply cost-complexity pruning to the CART tree.
#'   Helps prevent overfitting by removing branches that provide little predictive power.
#'   Default: TRUE
#'
#' @param cv_folds Integer number of cross-validation folds for pruning parameter selection.
#'   Must be at least 2 when pruning is enabled. Higher values provide more robust pruning.
#'   Default: 5
#'
#' @param monotonic_trend String specifying the desired monotonic relationship. Options:
#'   \itemize{
#'     \item \code{"auto"}: Automatically detect trend based on Spearman correlation (default)
#'     \item \code{"increasing"}: Enforce increasing WoE trend
#'     \item \code{"decreasing"}: Enforce decreasing WoE trend
#'     \item \code{"none"}: No monotonic constraint
#'   }
#'   Default: "auto"
#'
#' @param monotonic_mode String specifying the method for enforcing monotonicity. Options:
#'   \itemize{
#'     \item \code{"pava"}: Isotonic regression using PAVA algorithm (default)
#'     \item \code{"merge"}: Heuristic adjacent bin merging
#'   }
#'   Default: "pava"
#'
#' @param parallel Logical indicating whether to use parallel processing via OpenMP for multiple features.
#'   Requires OpenMP support in compilation. Speeds up processing of many features.
#'   Default: FALSE
#'
#' @param weights Optional NumericVector of observation weights for weighted binning. Length must equal
#'   number of rows in data. All weights must be non-negative. NULL indicates equal weights.
#'   Default: NULL
#'
#' @param special_vals NumericVector of special values to handle separately (e.g., -999, -888 for
#'   special codes). These values are isolated in separate bins when encountered.
#'   Default: empty vector (no special values)
#'
#' @param max_cat Integer maximum number of categories before automatic grouping for categorical variables.
#'   Must be at least 2. Categories beyond this limit are grouped based on event rate similarity.
#'   Default: 50
#'
#' @param rare_pct Numeric threshold for rare category grouping as proportion of total.
#'   Categories with frequency below this threshold are combined. Must be between 0 and 0.5.
#'   Default: 0.01 (1\%)
#'
#' @param verbose Integer controlling log verbosity level (0-3):
#'   \itemize{
#'     \item 0: Silent mode - no logs
#'     \item 1: Basic logs - key steps and results  
#'     \item 2: Detailed logs - includes algorithm progress
#'     \item 3: Debug logs - full algorithm details
#'   }
#'   Default: 0
#'
#' @return
#' Named list with one element per processed feature, each containing:
#' \itemize{
#'   \item \code{bins}: data.frame with columns:
#'     \itemize{
#'       \item \code{bin_id}: Sequential bin identifier (1, 2, 3, ...)
#'       \item \code{bin_label}: Formatted bin range/categories (e.g., "(-inf, 25.5]", "[25.5, 40.0]", "(40.0, +inf)")
#'       \item \code{total_count}: Total observations in bin
#'       \item \code{neg_count}: Count of negative class (Y=0)
#'       \item \code{pos_count}: Count of positive class (Y=1)
#'       \item \code{woe}: Weight of Evidence value for the bin
#'       \item \code{iv}: Information Value contribution of the bin
#'       \item \code{ks}: Kolmogorov-Smirnov statistic up to this bin
#'       \item \code{gini}: Gini coefficient for the bin
#'     }
#'   \item \code{total_iv}: Total Information Value for the feature
#'   \item \code{variable_type}: Detected type ("numeric_continuous", "numeric_discrete", "categorical", "boolean")
#'   \item \code{method_used}: Algorithm actually used for binning
#'   \item \code{n_bins}: Final number of bins created (including special bins if separate)
#'   \item \code{total_samples}: Total valid observations processed
#'   \item \code{event_rate}: Overall positive class rate in processed data
#'   \item \code{messages}: Processing messages or warnings
#'   \item \code{transform}: data.frame with original values and transformations:
#'     \itemize{
#'       \item \code{[feature]}: Original feature values
#'       \item \code{[feature]_bin}: Assigned bin labels
#'       \item \code{[feature]_woe}: Assigned WoE values
#'     }
#' }
#'
#' @section Algorithm Pipeline:
#' The complete binning process follows these steps:
#' \enumerate{
#'   \item \strong{Data Preparation}:
#'     \itemize{
#'       \item Detect variable type (numeric, categorical, boolean, date)
#'       \item Handle missing values according to \code{miss_policy}
#'       \item Process special values into separate bins
#'       \item For categorical variables: order by event rate and map to ordinal scale
#'     }
#'   \item \strong{CART Tree Construction}:
#'     \itemize{
#'       \item Sort data by feature value (numeric) or ordinal mapping (categorical)
#'       \item Build binary tree using best-first search with impurity gain criterion
#'       \item Respect \code{min_size} constraints during splitting
#'       \item Stop when reaching \code{max_bins} leaves or no beneficial splits
#'     }
#'   \item \strong{Tree Pruning} (if enabled):
#'     \itemize{
#'       \item Generate sequence of subtrees via cost-complexity pruning
#'       \item Select optimal subtree using cross-validation
#'       \item Ensure result respects \code{min_bins} and \code{max_bins} constraints
#'     }
#'   \item \strong{Bin Adjustment}:
#'     \itemize{
#'       \item Force minimum bin count if below \code{min_bins}
#'       \item Merge bins if above \code{max_bins}
#'       \item Apply monotonicity constraints if specified
#'     }
#'   \item \strong{Metric Calculation}:
#'     \itemize{
#'       \item Compute WoE and IV with Laplace smoothing
#'       \item Calculate KS statistics and Gini coefficients
#'       \item Generate transformation mappings
#'     }
#' }
#'
#' @section Special Cases Handling:
#' \itemize{
#'   \item \strong{Variables with \eqn{\le 2} unique classes}: Direct binning without optimization
#'   \item \strong{Date/Time variables}: Automatically skipped with warning message
#'   \item \strong{Unsupported types}: Skipped with appropriate message
#'   \item \strong{Constant variables}: Single bin created with warning
#'   \item \strong{Insufficient data}: Processing skipped with informative message
#' }
#'
#' @note
#' \itemize{
#'   \item WoE values are clipped to [-20, 20] for numerical stability
#'   \item Variables with 2 or fewer unique classes are processed directly without CART optimization
#'   \item Factor variables are processed as categorical with proper level handling
#'   \item Integer variables with \eqn{\leq 20} unique values are treated as discrete numeric
#'   \item Special values (-999, -888, etc.) can be isolated in separate bins
#'   \item Uses "(-inf,x)" and "[x,+inf)" notation for unbounded intervals
#'   \item Categorical bins combine multiple categories using \code{cat_sep} separator
#' }
#'
#' @references
#' \itemize{
#'   \item Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). Classification and regression trees. CRC press.
#'   \item Thomas, L. C., Edelman, D. B., & Crook, J. N. (2002). Credit scoring and its applications. SIAM.
#'   \item Hand, D. J., & Henley, W. E. (1997). Statistical classification methods in consumer credit scoring: a review. Journal of the Royal Statistical Society: Series A (Statistics in Society), 160(3), 523-541.
#'   \item Zeng, G. (2014). A necessary condition for a good binning algorithm in credit scoring. Applied Mathematical Sciences, 8(65), 3229-3242.
#'   \item Robertson, T., Wright, F. T., & Dykstra, R. L. (1988). Order restricted statistical inference. Wiley.
#' }
#'
#' @examples
#' \dontrun{
#' # Load credit data
#' data <- scorecard::germancredit
#' data$target <- ifelse(data$creditability == "good", 1, 0)
#'
#' # Basic binning with all defaults
#' result <- cart_woe(
#'   data = data,
#'   target_col = "target",
#'   feature_cols = c("duration.in.month", "credit.amount", "age.in.years")
#' )
#'
#' # Advanced binning with custom parameters
#' result_custom <- cart_woe(
#'   data = data,
#'   target_col = "target",
#'   feature_cols = names(data)[1:10],
#'   min_bins = 3,              # Minimum 3 bins
#'   max_bins = 8,              # Maximum 8 bins
#'   method = "cart+pava",      # CART with PAVA monotonicity
#'   miss_policy = "separate",  # Missing values in separate bin
#'   cat_sep = " | ",           # Use pipe separator for categories
#'   digits = 2,                # 2 decimal places in labels
#'   smooth = 1.0,              # Higher smoothing for WoE
#'   criterion = "entropy",     # Use entropy instead of Gini
#'   min_size = 0.03,           # Each bin at least 3% of data
#'   use_pruning = TRUE,        # Enable cost-complexity pruning
#'   cv_folds = 10,             # 10-fold CV for pruning
#'   monotonic_trend = "auto",  # Auto-detect monotonic trend
#'   monotonic_mode = "pava",   # Use PAVA for monotonicity
#'   parallel = TRUE,           # Parallel processing for speed
#'   special_vals = c(-999, -888), # Special codes to isolate
#'   max_cat = 25,              # Group if >25 categories
#'   rare_pct = 0.02            # Group categories <2% frequency
#' )
#'
#' # Extract results for a specific feature
#' duration_bins <- result$duration.in.month$bins
#' duration_iv <- result$duration.in.month$total_iv
#' duration_transform <- result$duration.in.month$transform
#'
#' # View binning results
#' print(duration_bins)
#' print(paste("Total IV:", duration_iv))
#'
#' # Check transformation
#' head(duration_transform)
#' }
#'
#' @export
cart_woe <- function(data, target_col, feature_cols, min_bins = 2L, max_bins = 5L, method = "cart", miss_policy = "separate", cat_sep = "%;%", digits = 4L, smooth = 0.0, criterion = "gini", min_size = 0.05, use_pruning = TRUE, cv_folds = 5L, monotonic_trend = "auto", monotonic_mode = "pava", parallel = FALSE, weights = NULL, special_vals = as.numeric( c()), max_cat = 50L, rare_pct = 0.01, verbose = 0L) {
    .Call(`_obwoe_cart_woe`, data, target_col, feature_cols, min_bins, max_bins, method, miss_policy, cat_sep, digits, smooth, criterion, min_size, use_pruning, cv_folds, monotonic_trend, monotonic_mode, parallel, weights, special_vals, max_cat, rare_pct, verbose)
}

